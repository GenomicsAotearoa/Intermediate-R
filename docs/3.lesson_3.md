# Tidy iteration and data storage

## The `map_*()` family

<!-- do I introduce the concept of functionals? -->

Learning objectives:

* Understand the anatomy of `map_*()` family of functions
* Understand anonymous functions
* Be able to use `map_*()` and its variations for iteration
* Efficiently manipulate list components

In the previous section, we have seen how to use loops to repetitively perform tasks. However, there is a suite of functions provided by the `purrr` package that can do that, but with fewer lines of code! These are the `map_*()` family of functions. These are functions that iterate through $n$ number of vectors (recall that lists are vectors).

<!-- needs image from purrr's cheat sheet -->

| `map_*()` functions | Number of vectors |
| ----------------- | ----------------- |
| `map_*()`           | 1                 |
| `map2_*`          | 2                 |
| `pmap_*()`          | > 2               |

??? `imap_*()` over a vector and it's index/name

    Besides the aforementioned `map_*()` family of functions, there is also `imap_*()` that iterates through a vector and it's index or name. Personally, I don't use it often as I think `map2_*()` and `pmap_*()` is more than sufficient (and more flexible).

The functions above always outputs a list. However, there are also modifiers you can use that can produce different kinds of outputs:

| Modifiers | Expected output type |
| --------- | -------------------- |
| `_chr`    | Character            |
| `_dbl`    | Double               |
| `_lgl`    | Boolean              |
| `_int`    | Integer              |

??? `_dfc` or `_dfr` 

    There are variations where the output is turned into a data frame using column- or row-bind. This is helpful if you are at the end of testing your code and you know exactly how the outputs need to be organised. However, for initial exploratory analysis, I find myself using `bind_rows()` or `bind_cols()` more often after the `map()`/`pmap()`.

Personally, I use `map()` and `pmap()` in most, if not all, my R scripts for its flexibility and applicability in a large variety of use cases. Your mileage and preference may vary, and I would love to hear about it!

Henceforth, I will use `map_*()` to mean all variations of map functions (including `pmap_*()` and `map2_*()`)

### Anatomy of `map_*()`

At a fundamental level, all `map_*()` functions require two arguments:

* Vector(s) (this includes lists and data frames)
* A function

```r
# One list/vector
map(vec, my_function)

# Two lists/vectors
map2(vec_A, vec_B, my_function)

# Multiple lists/vectors
pmap(list(vec_A, vec_B, vec_C), my_function)
```

**Anonymous functions and alternative notation**

In the previous lesson, we looked at how to assign a block of code that performs some task (calculate diversity indices) to a variable. This makes the object a function. However, what if you only need to use the function once, perhaps as a form of convenience? In that case you can write an anonymous function, which is to say a function that is not assigned a name. For example:

!!! r-project "code"

    ```r
    vec_A <- rnorm(10)
    vec_B <- rnorm(10, sd = 2.5)

    map2(vec_A, vec_B, function(x, y) {x * y})
    ```

The above function written inside the `map2()` call is not named. However, it produces the desired output as expected of a function. This is known as an anonymous function. 

There are also alternative notations (or shorthand) to specify functions. In base R, you can replace the word `function` with a backslash and it will behave the same way. 

!!! r-project "code"

    ```r
    a_function <- \(x, y) {x + y}

    a_function(1, 4)
    ```

In `purrr`, there is also another way to indicate functions via shorthand:

!!! r-project "code"

    ```r
    map(1:4, ~ .x ^ 2)
    map2(1:4, 2:5, ~ .x * .y)
    pmap(list(rnorm(4), rnorm(4), rnorm(4)), ~ ..1 * ..2 + ..3)
    ```

The above is equivalent to:

!!! r-project "code"

    ```r
    map(1:4, function(x) x ^ 2)
    map2(1:4, 2:5, function(x, y) x * y)
    pmap(list(rnorm(4), rnorm(4), rnorm(4)), function(x, y, z) x * y + z)
    ```

If you need to do small tasks with minimal lines of code (e.g., type/mode checks, cleaning, reshaping, etc.), anonymous functions in the `purrr` framework makes code more streamlined and readable. In practice, I often reserve named functions for code blocks that perform tasks that are

* Complex (e.g., myriad dependencies with other functions, multiple conditional statements, generalisable, etc.)
* Resource-intensive (for ease of benchmarking or troubleshooting where profilers can identify which line in the function block is using the most resources)
* Highly repetitive within or across scripts (i.e., repeatedly called in the same script or `source()` across multiple scripts) 

!!! question "`for` loop or `map_*()`?"

    In older versions of R, the general advice leaned towards using vectorised iterations such as `map_*()` (or `*apply()` family of functions from base R). This is due to the runtime of a `for` loop being slower (this is still the case if your iteration, input, intermediate processes/outputs, and final outputs are very large). New versions of R have fixed that and is now about on par with these vectorised functions. There is a specific case where a `for` loop is favoured:
    
    * **Uneven number of elements** `map2()` and `pmap()` requires that all lists in the arguments have the same number of elements. This is not a requirement in a `for` loop. You can iterate through as many lists of different sizes in as many nested constructs as you want! The trade-off is that it does take a little bit more set-up to allocate output sizes and how you would like the output to look.

    In many use cases, `map_*()` is likely sufficient. Some pros of using `map_*()` as opposed to a `for` loop are:

    * **Readability** The vectorised nature of the functions means that code is more concise. As all nested outputs can be stored as list specified within the function (anonymous or not) to be iterated, there is no need to remember indices!
    * **Control of output types** You can use the different variations of `map_*()` to control what types/mode of outputs you want. During interactive coding, this can help you troubleshoot potential problems early on. Unlike coercion, `map_*()` will not produce an output if the resultant object is not of the type specified. Thus, it ensures data type/mode consistency when the output is used for other downstream processes should you need to scale the input/output data.

To help us grasp how `map_*()` works, we will perform an exploratory analysis of beta diversity using our data set.

## Activity: Exploring beta diversity using `map_*()`

### 1. Reading multiple files

This is one use case of `map()` where I can load many files generated from a software all at once. There are instances that bioinformatic analyses can be run in parallel, thus suitable for array-type HPC jobs. If we planned ahead prior to running bioinformatic analyses and standardised all output filenames, we can combine string matching operations and iterations to load these files all at once!

Here, we are loading 2 UniFrac distance matrices:

* `unweighted_unifrac.dist` which is the community phylogenetic distance
* `weighted_unifrac.dist` which is the the same distance weighted by relative abundance 

!!! r-project "code"

    ```r
    # Find all files with the ".dist" suffix
    unifrac_files <- list.files(pattern = ".*.dist")
    # Map over all files found
    dist_matrix <- map(unifrac_files, read_tsv)
    # Name them based on the file prefix
    names(dist_matrix) <- str_remove(unifrac_files, ".dist") %>%
      str_to_lower()
    ```

Using the above code block, we've read in 2 files and stored them as a list called `unifrac_dist`. In the list, there are 2 data frames, each containing a variation of the UniFrac distance matrix. 

??? tip "Export list elements to the Global environment `.GlobalEnv`"

    Sometimes you might want to export the imported data into the working environment (this is almost always the global environment `.GlobalEnv`). You can achieve this by doing the following:

    !!! r-project "code"

        ```r
        # Let's use the elements in unifrac_dist as an example
        list2env(unifrac_dist, envir = .GlobalEnv)
        # Print everything in the global environment
        ls()
        ```

        !!! success "Output"

            ```
            [1] "asv"                "meta"               "taxa"               "unifrac_dist"       "unifrac_files"     
            [6] "unweighted_unifrac" "weighted_unifrac"
            ```
        
    Take note that in order for this to work properly, you do need to name each element in the list.

### Step 2: Appending a list

In microbial ecology, it is also common to compare different kinds of distances and dissimilarities. Remember, we still have our explorer's hat on! Other commonly used dissimilarities are the Jaccard index (based on binary data) and Bray-Curtis dissimilarities (based on relative abundance data). These are calculated directly based on the `asv` data using `vegan`'s function called `vegdist()`. Let's also append this to our existing list of distance matrices.

!!! r-project "code"

    ```r
    # Create another list of dissimilarity matrices
    trad_dist_names <- c("jaccard", "bray") %>% 
      set_names(.)
    trad_dist <- map(trad_dist_names, \(x) {
      # Transform the ASV table into a transposed numeric matrix
      data <- column_to_rownames(asv, "ASVID") %>% 
        t()
      # Convert matrix to presence-absence matrix for jaccard
      if (x == "jaccard") {
        data <- ifelse(data > 0, 1, 0)
      }
      # Calculate dissimilarities using the vector as arguments
      vegdist(data, method = x)
    })

    # Combine both lists
    dist_matrix <- append(trad_dist, dist_matrix)
    ```

In the last line, we use the `append()` function to add elements into our list. Depending on your needs, the order of which goes first may matter. Here, we've put the traditional dissimilarities before the UniFrac distances.

### Step 3: Selectively modifying elements in a list

Inspect your list after appending `trad_dist` into `dist_matrix`. We will do this using `purrr`'s set of functions that evaluate a predicate across a list.

!!! r-project "code"

    ```r
    every(dist_matrix, \(x) any(class(x) %in% "dist"))
    ```

    ??? success "Output"

        ```
        [1] FALSE
        ```

Let's breakdown the function above:

* `every()` is a `purrr` function that checks that every element of the list (i.e., `dist_matrix`) returns a TRUE for the predicate function `\(x) any(class(x) %in% "dist"))`
* `any(class(x) %in% "dist"))` checks if any of the elements have the class attribute of "dist" (an object can have more than one class attribute). It returns a TRUE if any of them is a "dist", and FALSE if none of them are "dist"

!!! note "What is a predicate function?"

    These are functions that evaluate whether a statement (written in the form of a function) is TRUE or FALSE. This can be an anonymous function like those written above, or a named function such as `is.numeric()`. The function must output a single boolean value to be considered a predicate function. `purrr` offers other predicate functions (see cheat sheet) that may be useful depending on your needs.

Recall that when we read the UniFrac distances into our list, it was a data frame and tibble (due to the intrinsic nature of `read_tsv()`). We need to convert them into "dist" data types without modifying the other elements. We can do this using `modify_if()`:

```r
dist_matrix <- modify_if(
  dist_matrix, 
  \(x) !any(class(x) %in% "dist"), 
  \(x) {
    column_to_rownames(x, "sample") %>% 
      as.dist()
})
```

Two anonymous functions! Let's look at the anatomy of the `modify_if()`:

```r
modify_if(<vector/list>, <predicate function>, <function>)
```

Using the function's anatomy as a guide, we can translate the `dist_matrix <- modify_if()` into plain English:

> Iterate through the list called `dist_matrix`. `modify_if()` the element in the list *does not* (`!`) contain `"dist"` in `any()` of its `class()` attribute by converting the `"sample"` column in the `data.frame` into row names, and then coercing the `data.frame` into becoming a distance matrix object using `as.dist()`.

### Step 4: Iteratively create visualisations

Note to self: 

1. Create PCoA and heatmaps based on distance matrices
2. Create ordination plots

In the previous section, we have created a list containing 4 different dissimilarity/distance matrices derived from the same data (the ASV table). These 4 measures all capture different aspects of the same data:

* Jaccard dissimilarity measures the extent of shared ASVs between samples based on the presence-absence data
* Bray-Curtis dissimilarity captures differences between samples based on ASV relative abundances
* Unweighted UniFrac distance captures the community-wide phylogenetic distance between samples
* Weighted UniFrac is similar to it's unweighted counterpart, but the metric is weighted by the relative abundance of ASVs

!!! note "Excuse the jargon!"

    That is a lot of microbial ecology jargon! However, I'd like to remind us that ecology here is the vehicle, not the message. The overall goal is to write fewer lines of readable and functionally transparent code while developing a systematic approach to exploratory data analyses. The latter includes trying different things to see the data in different light (using different metrics, visual techniques, etc.).

From that, we may have one big question: **Do all metrics tell the same story?** Let's try to answer that by visualising this data! Dissimilarity/Distance matrices are symmetric square matrices (meaning they have the same values on opposite sites of the diagonal and have the same number of rows and columns). This can be represented directly using a heatmap.

Let's create 4 heatmaps, using a single code block:

```r
map2(dist_matrix, names(dist_matrix), \(D, nm) {
  heatmap(
    as.matrix(D), 
    scale = "none",
    hclustfun = \(x) hclust(x, method = "ward.D2"),
    col = viridis::viridis(2^8),
    xlab = str_to_title(nm), 
    margins = c(5, 5)
  )
})
```

??? tip "`map_*()` alternative: `walk()`"

    There is another set of functions: `walk()`, `walk2()`, and `pwalk()` that mimics what `map_*()` does, but is primarily used for producing side effects. This is helpful for eyeballing plots and graphs interactively as the outputs do not crowd the terminal, but a figure is produced. You can replace the `map2()` above with `walk2()` to see the difference in output.

From the heatmaps, we observe that:

* All dissimilarity/distance metrics consistently show 2 groupings of samples, one muddy and one sandy
* Dissimilarity/distance metrics that is weighted by relative abundance captures the differences between groups of muddy and sandy samples better

Although there were consistent groupings, heatmaps paired with dendrogram is perhaps not sensitive enough to capture some the smaller differences between samples. Let's use another technique, the principle coordinate analysis, to visualise the dissimilarities between our samples.

```r
# Reduce the number of dimensions using principle coordinates
PCOA <- map(dist_matrix, cmdscale)

# Plot the results in 2D as a panelled figure
par(mfrow = c(2, 2))
map2(PCOA, names(PCOA), ~ ordiplot(.x, main = .y, type = "text", display = "sites"))
```

## Flexible tables using tibbles

Learning objectives:

* Understand the difference between tibbles and data frames
* Be able to create a tibble
* Be able to manipulate data to create nested data
* Store and manipulate lists within a tibble

### Data frames, matrices, and... tibbles?

Until now, we have encountered 2 kinds of tabular (or 2-dimensional) data: 

* Data frames (used here and in Introduction to R)
* Matrices  (used here to make our heatmaps)
 
Here, we will use another kind of tabular data storage method: tibbles. However, before introducing the tibble, we should ask ourselves: What is the difference between a `data.frame` and a `matrix`?Recall that data frames are a special kind of list, where each column is a variable containing one type of data and all variables in this list have the same number of elements. In addition, different columns in the data frame can be of a different data type. You can check this with the `asv` or `taxa` data that we loaded. I'm using `taxa` here for brevity of output.

```r
# Check that taxa is a data frame
is.data.frame(taxa)
# Check that columns can be of different data types
map(taxa, mode)
```

```
[1] TRUE

$Feature_ID
[1] "character"

$Taxon
[1] "character"

$Confidence
[1] "numeric"
```

If you are familiar with checking data in R, you know we could have also used `str()` to determine each column's data type. Here, I used `map(asv, mode)` to show that it is a special case of list, where it naturally iterates through each column as if iterating through vectors stored in a list.

What about a matrix? It is also a 2-dimensional data structure, so what is different about it? Well, matrices are special cases of vectors. Remember that all elements in a vector must have the same data type? This is true of matrices. Matrices always have a `dim` (read dimension) attribute of 2. They can also have `dimnames` (read `row.names` and `col.names`). To illustrate this, let's use `asv`.

```r
# Check data type for entire table
str(as.matrix(asv))
```

```
chr [1:4031, 1:22] "88c2f5ea8fd2f13cbd43e933049707c9" "bab6b4a509dd01b93cbbf78e40616533" ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:22] "ASVID" "AS1A1" "AS1A2" "AS1A3" ...
```

That's quite an unusual result, but let's see what is going on line by line:

1. Note that the first line of the output shows that it is a `chr` mode of data. The ranges within `[]` show that we have 4031 indices in dimension 1 (rows) and 22 indices in dimension 2 (columns). It also outputs the first few elements, which are the ASVIDs.
2. It is also telling us there is an attribute (`attr`) called `"dimnames"` which is a list of 2 vectors:
   1. a `NULL` vector (we do not have row names, hence `NULL`)
   2. a character vector with 22 elements (these are our column names)

You might ask: why is it a character matrix? By default, if R encounters mixed data types, it will attempt to coerce everything as characters. We have encountered the function `column_to_rownames()` above, let's use that to turn the `ASVID` column into row names, coerce the data into a matrix, and then check the data using `str()`.

```r
column_to_rownames(asv, "ASVID") %>% 
  as.matrix() %>% 
  str()
```

```
 num [1:4031, 1:21] 45 81 70 35 61 25 16 43 26 32 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:4031] "88c2f5ea8fd2f13cbd43e933049707c9" "bab6b4a509dd01b93cbbf78e40616533" "24b484137e882a5c77ea93b4574332e1" "5d112573567f99604339d5d6b92bdea9" ...
  ..$ : chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
```

That's a subtly different output. We see that it is now numeric with 4031 and 21 indices in dimensions 1 and 2, respectively. There are also row names which are our ASVIDs and column names.

Okay, now you know what data frames and matrices really are, what has that got to do with tibbles? This is about flexibility. Notice that with data frames and matrices, you are really limited by what kind of data you can store: atomic data types. What if you have analyses that require the output of lists and arrays? Perhaps you are transforming the data multiple times and need to save the intermediate results for alternative analyses, are you going to crowd your `.GlobalEnv`? Recursively and iteratively create more nested lists like Matryoshka dolls? This is where tibbles come in.

Tibbles are an evolution of data frames, but instead of being smarter at guessing data, they're lazier. They are lazy because they:

!!! note

    === "Do not enforce data types"

        Think of this like digital shelves. You can store whole vectors, lists, arrays/matrices, even data frames within each cell!
  
    === "Do not adjust variable names (think column names)"

        Have you ever had comma- or tab-delimited files where headers are named with spaces like below?

        ```
        Sample    Salinity    Mud content    Organic carbon
        ```

        These will inevitably trigger either a warning, or depending on the characters used, even an error when importing data. If imported as a tibble, R will not complain, and you can refer to each column by surrounding it with backticks, like so:

        ```r
        tbl$`Mud content`
        ```

    === "Lazily and sequentially evaluate arguments"

        This is a unique feature of tibbles that makes it easy to create and populate tabular data, especially if there are dependencies between variables. Consider this:

        We want to create a table where column A is a vector of integers, and column 2 is a calculation dependant on column B.

        If we were to create this using `data.frame()`, the code will look like this:

        ```r
        # Create data frame with column A
        df <- data.frame(
          A = rpois(10, 4)
        )
        # Create column B in data frame using values from column A as the mean
        df$B <- rnorm(n = nrow(df), mean = df$A)

        df
        ```

        ```
           A        B
        1  5 5.613719
        2  7 6.700890
        3  3 1.993785
        4  4 4.312433
        5  3 3.292354
        6  2 3.489764
        7  3 4.938117
        8  5 5.246944
        9  4 5.747918
        10 5 4.207845
        ```

        Now let's look at how to create a similar table using `tibble()`:

        ```r
        tb <- tibble(
          A = rpois(10, 4),
          B = rnorm(n = length(A), mean = A)
        )

        tb
        ```

        ```
        # A tibble: 10 × 2
               A     B
           <int> <dbl>
         1     7  8.37
         2     4  4.06
         3     6  6.78
         4     1  1.02
         5     3  1.42
         6     7  5.89
         7     3  1.69
         8     2  2.53
         9     6  5.33
        10     3  4.01
        ```

        Notice how much cleaner (and less typing) that was? It evaluates every argument based on whether if something was created in the lines before it.

One other thing you may notice is how data is printed. The example above prints `df` and `tb`, but the unique printed output of tibbles becomes really useful when looking at larger tables. Compare the following:

```r
as.data.frame(asv)
as_tibble(asv)
```

Notice that with `as.data.frame(asv)` the printed output is quite voluminous and it ends with a `[ reached 'max' / getOption("max.print") -- omitted 3986 rows ]` warning. However, with `as_tibble(asv)`, the result is cleaner and more informative. It's like `str()`, but tabular.

Tibbles are an essential part of how data is stored in the `tidy` framework. When importing data using functions from the `readr` package, most of the time, the resulting data object will be a tibble. Personally, I find tibbles to be highly useful when doing interactive work (e.g., analyses where I have to supervise intermediate outputs, plotting and comparing multiple outputs). Once I know what outputs I want and the code I will use *en masse*, I will often write a new script that does just that to streamline the process. That way, I have a script that describes my exploratory process (usually tibbles involved) and another that is written to produce a certain outcome (usually matrices involved).

## Activity: Statistical tests using tibbles

For this activity, we will combine what we have learned from `map_*()` with the flexible data storage methods of tibbles to perform multiple tests. We will also attempt this activity under the assumption that this is a completely new environment, where you have only read in `asv` and `meta`. Here, we aim to explore how communities vary relative to each other and determine the relative effects of environmental variables in `meta` drive this variation. In ecology, this is known as $\beta$-diversity. Here is an overview of the activity:

1. Set up tibbles by obtaining relevant distance matrices
2. Generate PCoA ordinations
3. Test concordance between community and environmental variation
4. Test which environmental variable drives dissimilarity between communities

### Step 1: Set up tibbles

Let's start small by setting up only the distance matrices. We will also include another metric, the (robust) Aitchison distance.

```r
beta_diversity <- tibble(
  metric = c("jaccard", "bray", "robust aitchison", "unweighted unifrac", "weighted unifrac"),
  D_mat = map(metric, \(d) {
    # Transform asv into a numeric matrix as preferred by vegan
    data <- column_to_rownames(asv, "ASVID") %>% 
      as.matrix() %>% 
      t()
    
    # Conditional statement for Jaccard index: transform to presence-absence
    if (d == "jaccard") {
      data <- decostand(data, method = "pa")
    }
    
    # Calculate distance matrices for non-UniFrac based distances
    # vegdist supports partial matching!
    if (!str_detect(d, "unifrac")) {
      distance <- vegdist(data, method = str_sub(d, 1, 3))
    } else {
      # This is where consistent file naming upstream really pays off!
      filename <- paste0("tables/", str_replace(d, " ", "_"), ".dist")
      distance <- read_tsv(filename) %>% 
        column_to_rownames("sample") %>% 
        as.dist()

      # All distances calculated by vegdist has a method attribute
      # This is how vegan tracks data transformations
      # Given the UniFrac distances were read in and not calculated, we will
      #   manually add it into the data
      attr(distance, "method") <- str_replace(d, " ", ".")
    }
    
    distance
    
  })
)
```

**Inspecting the data**

We should take a look at what we've just created. Notice that if you directly call it, it just prints the types of data in the `D_mat` column:

```
# A tibble: 5 × 2
  metric             D_mat       
  <chr>              <list>      
1 jaccard            <dist [210]>
2 bray               <dist [210]>
3 robust aitchison   <dist [210]>
4 unweighted unifrac <dist [210]>
5 weighted unifrac   <dist [210]>
```

Let's also inspect what each item in D_mat contains:

```r
walk(beta_diversity$D_mat, str)
```

```
 'dist' num [1:210] 0.772 0.712 0.727 0.748 0.709 ...
 - attr(*, "Size")= int 21
 - attr(*, "Labels")= chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
 - attr(*, "Diag")= logi FALSE
 - attr(*, "Upper")= logi FALSE
 - attr(*, "method")= chr "jaccard"
 - attr(*, "call")= language vegdist(x = data, method = str_sub(d, 1, 3))
 'dist' num [1:210] 0.67 0.545 0.552 0.583 0.556 ...
 - attr(*, "Size")= int 21
 - attr(*, "Labels")= chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
 - attr(*, "Diag")= logi FALSE
 - attr(*, "Upper")= logi FALSE
 - attr(*, "method")= chr "bray"
 - attr(*, "call")= language vegdist(x = data, method = str_sub(d, 1, 3))
 'dist' num [1:210] 27.1 29.1 30.8 31.4 31.3 ...
 - attr(*, "Size")= int 21
 - attr(*, "Labels")= chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
 - attr(*, "Diag")= logi FALSE
 - attr(*, "Upper")= logi FALSE
 - attr(*, "method")= chr "robust.aitchison"
 - attr(*, "call")= language vegdist(x = data, method = str_sub(d, 1, 3))
 'dist' num [1:210] 0.412 0.345 0.388 0.392 0.357 ...
 - attr(*, "Labels")= chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
 - attr(*, "Size")= int 21
 - attr(*, "call")= language as.dist.default(m = .)
 - attr(*, "Diag")= logi FALSE
 - attr(*, "Upper")= logi FALSE
 - attr(*, "method")= chr "unweighted.unifrac"
 'dist' num [1:210] 0.0664 0.0497 0.0884 0.0763 0.0692 ...
 - attr(*, "Labels")= chr [1:21] "AS1A1" "AS1A2" "AS1A3" "AS1B1" ...
 - attr(*, "Size")= int 21
 - attr(*, "call")= language as.dist.default(m = .)
 - attr(*, "Diag")= logi FALSE
 - attr(*, "Upper")= logi FALSE
 - attr(*, "method")= chr "weighted.unifrac"
```

Here, I'm using `walk()` as I only want the printed output as there are no transformations or calculations involved. `str()` also doesn't output anything beyond the printed items, so using `walk()` is the natural choice here. You can try this using `map()` as well, but the output will be slightly odd as there is no output per item.

Based on the output, we can tell that the distance outputs from `vegdist()` was successful and ran as expected, given that the `method` attributes are based on the metric we requested. We also see consistency with the number of elements for `Labels` (sample IDs), `Size` (number of samples), `Diag` (the matrix diagonal), and `Upper` (the upper triangle of the matrix).

!!! note "Why check attributes?"

    Attributes is how R identifies certain properties of an object. For example, remember that a matrix is a vector with `dim` of 2? Well, this is an attribute of matrix, where it has a 2 dimensions with lengths of *m* and *n*. In many cases, you can ignore checking for attributes unless you need to troubleshoot something. However, some packages use attributes to internally validate the kinds of data it is working with. For packages performing complex calculations or data transformations, this ensures consistency in data types throughout the pipeline/workflow. This is especially important for packages that have an abundance of options at each step (i.e., vegan) and that different upstream options may need to be handled differently.

### Step 2: Mutate tibble

Now that we have our distance matrices set up, let's generate two additional columns based on our distances:

* Unconstrained ordination using PCoA (principle coordinate analysis)
* Constrained ordination using dbRDA (distance-based redundancy analysis)

We will also extract relevant values from our test to make the results easily accesible.

```r
beta_diversity <- beta_diversity %>% 
  mutate(
    PCoA = map(D_mat, cmdscale),
    Mantel.test = map(D_mat, \(D) {
      # Coerce environmental data into matrix
      env_mat <- column_to_rownames(env_data, "sample") %>% 
        as.matrix()
      # Generate Aitchison distance from environmental matrix
      env_dist <- vegdist(env_mat, method = "ait")
      # Correlate between environmental and community distances
      mantel(D, env_dist, method = "spearman")
    }),
    Mantel.correlation = map_dbl(Mantel.test, "statistic"),
    Mantel.pvalue = map_dbl(Mantel.test, "signif")
  )
```








