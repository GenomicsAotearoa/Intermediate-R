{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#_1","title":"Home","text":""},{"location":"#preamble","title":"Preamble","text":"<p>In this workshop, we will be learning a few techniques to achieve the following goals:</p> <ul> <li>Write fewer lines of code</li> <li>Scale analyses</li> <li>Understand how to put multiple concepts together</li> <li>Make code predictable and readable.</li> </ul> <p>The topics we will cover here are:</p> <p>1. Manipulating strings 2. Managing and manipulating relational data 3. Writing functions 4. Using iterations 5. Understanding data structures</p> <p>Although the lessons are structured in a modular fashion, these techniques complement each other in order to help us achieve the goals above.</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<p>To fully engage with the material in this workshop, you will need:</p> <ul> <li> <p> An introductory knowledge of R</p> <ul> <li>Variable assignment</li> <li>Mathematical and Boolean operators</li> <li>Data modes</li> <li>Importing data</li> </ul> </li> <li> <p> Some familiarity with the <code>tidyverse</code> dialect of R code as shown here:</p> <ul> <li><code>mutate()</code> and <code>filter()</code></li> <li><code>group_by()</code> and <code>summarise()</code></li> <li><code>%&gt;%</code></li> </ul> </li> <li> <p> Familiarity with lists:</p> <ul> <li>Understand the difference between <code>[</code> and <code>[[</code> when subsetting a list</li> <li>Understand what nestedness means in a list</li> </ul> </li> </ul>"},{"location":"0.prep/","title":"An introduction to the data","text":"<p>The data that we will be working with today came from sequences generated for this study. Our data is a subset of the sequence data reanalysed using a popular amplicon sequence processing pipeline QIIME2. You can explore how this data was generated in the supplementary materials.</p> <p>This data consists of 21 sediment samples taken from the Waiwera Estuary (North of Auckland). It was part of a study that looked at how prokaryotic communities and their nitrogen cycling fractions changed along a gradient of mud contents. Alongside sequence data, environmental variables such as mud content (percentage dry weight of clays and silt) and various chemical data such as carbon, nitrogen, sulfur and phosphorus were also measured.</p> <p>This dataset follows a typical microbial ecology study structure, and has three distinct data files:</p> <ul> <li><code>asv</code>: a count of organisms (here, ASV or amplicon sequence variants) per sample</li> <li><code>tax</code>: describes the taxonomic lineage of ASVs</li> <li><code>env</code>: sample metadata describing various non-biological measurements obtained from the samples</li> </ul> <p>The data set, and any inference that is generated from it, is not our primary interest today. Here, it is used as an example to illustrate the functionality of the concepts in the lessons and as a convenient source of data.</p>"},{"location":"0.prep/#load-packages-and-import-data","title":"Load packages and import data","text":"<p>code</p> <pre><code>library(vegan)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\n\nasv &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/asv_table.tsv\")\nenv &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/env_table.tsv\")\ntax &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/taxonomy.tsv\")\n</code></pre>"},{"location":"1.string_manipulation/","title":"String manipulation","text":"<p>Learning objectives</p> <ul> <li>Construct regular expressions</li> <li>Find and count patterns in strings</li> <li>Replace and subset (sub-)strings</li> <li>Concatenate strings</li> </ul>"},{"location":"1.string_manipulation/#cant-i-change-it-in-excel","title":"Can't I change it in Excel?","text":"<p>Sure, go ahead for small datasets and ad hoc analyses. However, can we be confident of being consistent for large datasets? Did we miss a few cells? Did we introduce extra punctuation? Can Excel handle the amount of data? A good understanding of string manipulation can be useful for data cleaning (as mentioned before) and come in handy for many situations. For example:</p> <ul> <li>Extracting statistics and formatting them for plots</li> <li>Using sub-strings as table keys</li> <li>Pattern extraction</li> <li>Manipulating URLs</li> </ul> <p>In this lesson, we will learn how to use functions from the <code>stringr</code> package to manipulate and extract strings from taxonomic lineages in the <code>tax</code> data frame.</p> <p>Biological strings</p> <p>At the fundamental level, all biological sequences (i.e., DNA, RNA, and amino acid) are strings. While the functions covered here can handle sequences, more is needed to extract biological relevance from them. When we think about biological sequences such as nucleic or amino acid sequences, we are interested in the relationships within and between strings (e.g., translations, alignments, k-mers, secondary structures, etc.). R has some packages that can handle such operations (e.g., <code>seqinr</code>, <code>Biostrings</code>, <code>DADA2</code>; see other packages in the Bioconductor repository). For those analyses, other programming languages offer better support and computational efficiencies.</p>"},{"location":"1.string_manipulation/#regular-expressions","title":"Regular expressions","text":"<p>A common thread (pun intended) in all string-based operations is regular expression (AKA regex or regexp). You probably have some experience with it if you have used <code>sed</code> or <code>awk</code> in <code>bash</code>. Formally, regular expressions are characters that represent finite sets of characters and the operations that can be performed on them. It is helpful to think of them as patterns.</p> <p>Consider this example:</p> <p><code>[0-9]+.*_(log|chk)\\\\.txt</code></p> <p>The pattern is read from left to right. Let's break it down:</p> <ul> <li><code>[0-9]</code> is a set of numbers from 0 to 9. The square brackets (<code>[]</code>) are meta-characters that allow matches to ranges (as in this example) or individual characters.</li> <li><code>+</code> indicates that the pattern preceding it should occur \\(\\ge\\) 1 time(s)</li> <li><code>.*</code> means to match anything <code>.</code> \\(\\ge\\) 0 times <code>*</code></li> <li><code>_</code> is just an underscore</li> <li><code>(log|chk)</code> means to match either <code>log</code> or <code>chk</code> as a word. The <code>|</code> functions as an OR operator and the round brackets <code>()</code> indicate that the patterns inside them must be interpreted as a word.</li> <li><code>\\\\.</code> matches a literal dot <code>.</code> where the double backslashes <code>\\\\</code> mean to \"escape\" the pattern following it</li> </ul> <p>Based on the pattern/expression, we can safely assume that it should match a text file with a date or process number as a prefix, followed by some description and the output type. For example the expression would find <code>12345_some_log.txt</code> but would not find <code>file.txt</code> because it does not start with any digits.</p> <p>In most cases, learning to construct regular expressions is based on trial and error and many Google searches. To soften the learning curve, the <code>stringr</code> team compiled a helpful cheat sheet we can reference.</p>"},{"location":"1.string_manipulation/#wrangling-taxonomy","title":"Wrangling taxonomy","text":""},{"location":"1.string_manipulation/#obtaining-microbial-taxonomy-from-dna-sequences","title":"Obtaining microbial taxonomy from DNA sequences","text":"<p>A major aim of microbial ecology is the identification of populations across an environment. We do that by sequencing the amplicon of the 16S small subunit ribosomal RNA gene, the standard taxonomic marker. Then, sequences are clustered based on sequence similarity (to reduce redundancy and improve computational efficiency) and then assigned a taxonomic lineage using a classifier that compares our sequence data with those in a reference database (popular options are SILVA and Greengenes 2). Depending on how similar and well-represented the sampled sequences are to those in the database, our sequences will be assigned names and ranks ranging from domain to species.</p>"},{"location":"1.string_manipulation/#inspecting-taxonomy","title":"Inspecting taxonomy","text":"<p>Let us begin by inspecting what our taxonomy looks like.</p> <p>code</p> <pre><code>head(tax$Taxon)\ntail(tax$Taxon)\n</code></pre> What's in a taxonomy <ul> <li>A semicolon <code>;</code> separates the ranks</li> <li>Ranks are given a single-letter prefix followed by <code>__</code></li> <li>Ranks are unevenly assigned. Some are identified down to species level, while only phylum is known in others.</li> </ul>"},{"location":"1.string_manipulation/#detecting-and-extracting-patterns","title":"Detecting and extracting patterns","text":"<p>Some initial questions when inspecting the above taxonomy are:</p> <ol> <li>How well characterised are our sequences?</li> <li>Did we manage to retrieve biologically important taxa?</li> </ol> <p>We can answer those questions using pattern detection.</p> <p>1. How well characterised are our sequences?</p> <p>Let's apply a heuristic and answer a simpler question: How many sequences were classified at each taxonomic rank (species, genus, family, order, class, phylum)? If there are large numbers of sequences that were only identified at higher taxonomic ranks, the system we are studying may harbour lots of novel microbial populations.</p> <p>code</p> <pre><code>str_detect(tax$Taxon, \"s__\") %&gt;%\n  sum()\n</code></pre> <p>In the code above, we used the function <code>str_detect()</code> to find the species prefix <code>s__</code> in the <code>Taxon</code> column. The output of <code>str_detect()</code> is a logical/boolean vector. Thus, we use <code>sum()</code> to count the number of <code>TRUE</code> statements.</p> <p><code>stringr</code> syntax</p> <p>Most functions in the <code>stringr</code> package accept arguments in this order:</p> <pre><code>str_&lt;name&gt;(&lt;vector&gt;, &lt;pattern&gt;, ...)\n</code></pre> Question <p>What is the proportions of ASVs that have been assigned a lineage with rank of genus and phylum?</p> Solution <pre><code>sum(str_detect(tax$Taxon, \"g__\")) / nrow(tax)\nsum(str_detect(tax$Taxon, \"p__\")) / nrow(tax)\n</code></pre> <p>It looks like our sequences are well characterised from the rank of genus and up.</p> <p>2. Did we manage to retrieve biologically important taxa?</p> <p>An ecosystem service that estuaries provide is nitrogen removal (via denitrification). These are usually performed by prokaryotes spanning the Bacterial and Archaeal domains. Their metabolic activity ensures that excess nitrogen is removed in gaseous form and thus prevents eutrophication. The starting substrate for denitrification is nitrate. Thus, reduced nitrogen must first be oxidised via nitrification. Two communities are involved in the conversion from reduced to oxidised nitrogen: </p> <ul> <li>Ammonia oxidisers (usually has the prefix \"Nitroso\" in their taxonomy) </li> <li>Nitrite oxidisers (usually has the prefix \"Nitro\" in their taxonomy)</li> </ul> <p>Lets find out if we managed to sample any of them.</p> <p>We will first need to subset the vector to retain those that have \"Nitro\" in their names. We will do this using <code>str_subset()</code>.</p> <p>code</p> <pre><code>nitro &lt;- str_subset(tax$Taxon, \"__Nitro\")\nlength(nitro)\n</code></pre> <p>We will also take a finer look at their lineages so we can get a better idea of which community they belong to.</p> <p>code</p> <pre><code>str_replace(\n  nitro,\n  \"d__([^;]+);.*(Nitro[a-z]+).*\",\n  \"\\\\1, \\\\2\"\n) %&gt;% \n    unique()\n</code></pre> <p>The code above is quite complicated. Let's break it down.</p> <ul> <li>The function <code>str_replace()</code> is a flexible function that helps us extract and replace substrings depending on how the regex was constructed.</li> <li><code>d__([^;]+);</code> looks for the sub-string <code>d__</code> followed by anything that is not a semicolon <code>[^;]+</code> more than once. The regex <code>[^&lt;some_pattern&gt;]</code> means to match anything that is NOT <code>&lt;some_pattern&gt;</code>. The round brackets <code>()</code> \"captures\" or \"saves\" the matches within it for replacement. This is followed by a semicolon (our rank separator) which is not captured but is present in the vector.</li> <li><code>.*(Nitro[a-z]+).*</code> As we do not know at which rank the first instance of \"Nitro\" will appear, the regex <code>.*</code> will match anything <code>.</code> more than 0 times <code>*</code>. At the first \"Nitro\" it encounters, we will also look for any subsequent letters in small case ranging from 'a' to 'z' as represented by <code>Nitro[a-z]+</code>. Anything after that can be matched but is not captured.</li> <li>The last argument in the function specifies how the replacement string should look like. <code>\\\\1, \\\\2</code> replaces the output with the two patterns we captured separated by a comma and a space. Patterns are captured sequentially and must be referenced in the order which they appear in the original string. Therefore, if we wanted the \"Nitro\" part to be in front, we would reverse the order to <code>\\\\2, \\\\1</code>.</li> </ul> <p>In case of failure...</p> <p>If <code>str_replace()</code> cannot find matches for the given pattern, it will return the original string. This is a safety mechanism. We can choose to filter it out later if necessary.</p> <p>Run the following and see for yourself:</p> <p>code</p> <pre><code>str_replace(nitro, \".*p__([^;]+).*g__([^;]+).*\", \"\\\\1, \\\\2\") %&gt;% \n  unique()\n</code></pre> <p>Other useful functions</p> <p>The functions <code>str_detect()</code> and <code>str_replace()</code> were the focus of this lesson for their flexible application and ease of visualising how regex and pattern capture works. Over the years, I have also found the functions below to be useful</p> <p>Concatenation: <code>str_c()</code>, <code>paste()</code></p> <p>Concatenates any number of string vectors per element (via the <code>sep =</code> argument) and/or across elements (via the <code>collapse =</code> argument)</p> <p>code</p> <pre><code>str_c(fruit[1:5], words[1:5], sep = \", \")\nstr_c(fruit[1:5], collapse = \"||\")\n</code></pre> <p>Interpolation: <code>str_glue()</code></p> <p>Evaluates expressions within <code>{}</code>, and then interpolate and concatenate them as strings. Very useful for programmatic use.</p> <p>code</p> <pre><code>prop_species &lt;- sum(str_detect(tax$Taxon, \"s__\")) / nrow(tax)\n\nprint(\n  str_glue(\"QIIME2 classified {prop_species} of ASVs down to species level.\")\n)\n</code></pre> <p>Separation by delimiter: <code>str_split()</code></p> <p>Splits a string based on a provided delimiter and returns a character vector. If a character vector of length &gt; 1 is provided as input, it will return a list of character vectors with each list element split based on the delimiter.</p> <p>code</p> <pre><code>str_split(nitro, pattern = \"; \")\n</code></pre> <p>Whitespace trimming: <code>str_trim()</code></p> <p>Removes whitespace at each end of the string. Very useful during data cleaning to make sure there are no trailing whitespaces that prevents downstream analyses.</p> <p>code</p> <pre><code>str_trim(\"   this has blanks   \")\n</code></pre> <p>I highly recommend playing with the functions above to get a feel for how they work. <code>stringr</code> has some built-in character vectors that you can use on-the-fly as test cases: <code>fruit</code>, <code>words</code>, and <code>sentences</code>.</p>"},{"location":"2.relational_data/","title":"Relational data","text":"<p>Lesson objectives</p> <ul> <li>Understand the concepts in relational data</li> <li>Use <code>left_join()</code> to join two tables based on shared keys</li> <li>Understand the different <code>join</code> functions</li> </ul>"},{"location":"2.relational_data/#a-brief-introduction-to-relational-data","title":"A brief introduction to relational data","text":"<p>Data analysis rarely involves only one data source. In many cases, insightful conclusions (and hypotheses) can only be gleaned and built by combining data from multiple sources. Collectively, this type of data is termed relational data, where the relationships between data points are of primary importance. Relational data is widespread in biology and you have probably encountered and generated it during your research. The table below outlines some cases where relational data can arise from:</p> Case Example scenario Subjecting the same data to different types of analyses Based on an amino acid sequence, predict signal peptide sequences, active site motifs, and secondary structures using different software. Collecting and analysing different data from the same sample. Correlate chemical measurements of different metabolites with sequence variants within a population. Comparing your data with data available in the literature or public databases. Identify homology between your set of sequences and those in NCBI\u2019s RefSeq and then query the sequence\u2019s role in metabolism via gene ontology. <p>Almost all modern databases rely on concepts derived from relational data. For example, searching for something in NCBI will return matches across various databases (e.g., Gene, Protein, Nucleotide, etc.). When you access one of the records you will see links to other NCBI databases for that record (e.g., Taxonomy, Conserved Domains, BioProjects, etc.). Moreover, the use of controlled vocabulary across biological databases (e.g., KEGG orthology (KO), enzyme commission (EC) numbers, transporter classification (TC) numbers, GO IDs, etc.) has made it easier to cross-reference information stored in other databases.</p>"},{"location":"2.relational_data/#the-keys-to-relational-data","title":"The key(s) to relational data","text":"<p>An essential characteristic of relational data is that data across multiple tables are connected via keys. You can think of keys as IDs. When analysing tabular biological data there is often a sequence ID (almost always based on the FASTA header) and other accompanying information (e.g., prediction scores, alignment scores, and other statistics). Let\u2019s say we annotated this sequence using BLAST and set it to produce a tabular output (<code>-outfmt 6</code>). BLAST would use the sequence header as the query ID/accession, and there would be another subject ID/accession column for hits in the database. This subject ID can then be used as a key to access other information kept in another table, perhaps taxonomic lineage, functional information, or even other accessions/IDs/keys to other records maintained in other databases.</p>"},{"location":"2.relational_data/#environmental-distribution-of-putative-ammonia-oxidising-taxa","title":"Environmental distribution of putative ammonia oxidising taxa","text":"<p>In the previous lesson, we found that there were biogeochemically important ammonia oxidising and nitrifying taxa in our samples. Here, we want to visualise how these taxa change with environmental conditions. This can give us some idea of the conditions that enrich or constrain these important taxa. Before visualisation, we will need to prepare the data used as input for <code>ggplot2</code>. Given that we are interested in environmental conditions of a taxanomically narrow functional guild, we will need to combine information from all three data: <code>asv</code>, <code>env</code>, and <code>tax</code>.</p> <p>Relationships between tables</p> <ul> <li><code>asv</code> and <code>tax</code> are related by the same hash (md5 checksum) key, but they are named <code>ASVID</code> in <code>asv</code> and <code>Feature_ID</code> in <code>tax</code>.</li> <li><code>asv</code> and <code>env</code> are related based on column names in <code>asv</code> and <code>sample</code> column in <code>env</code>.</li> </ul> <p>This is a graphical representation of their relationship with each other:</p> <p></p>"},{"location":"2.relational_data/#joining-tables-using-dplyr","title":"Joining tables using <code>dplyr</code>","text":"<p>To help us combine tabular data in R, we will use a set of <code>*_join()</code> functions from the <code>dplyr</code> package. Join functions can be used to add columns from one dataset to another dataset. If you are familiar with the relational database management system SQL, you should feel comfortable with these functions\u2019 relational operations. These functions will return a <code>NA</code> for non-matching keys or missing observations while preserving valid joins for other observations.</p> <p>Based on the previous lesson, we know that ammonia oxidisers often carry the \"Nitroso-\" prefix. The only table with that information is <code>tax</code>. To begin, we will subset that table to the relevant taxa and remove the <code>Confidence</code> metric.</p> <p>code</p> <pre><code>amo &lt;- filter(tax, str_detect(Taxon, \"Nitroso\")) %&gt;%\n  select(-Confidence)\n</code></pre> <p>We then need to add count per ASV per sample information from <code>asv</code>. To do this, we will use the <code>left_join()</code> function:</p> <p>code</p> <pre><code>amo_abd &lt;- left_join(amo, asv, by = join_by(\"Feature_ID\" == \"ASVID\"))\n</code></pre> <p>Quite literally, the function joins the table on the right to the table on the left. We also use the <code>by =</code> argument and the <code>join_by()</code> function to specify which columns are keys in both tables. In layperson English, we can read the above as \"join the <code>asv</code> table to the <code>amo</code> table where the columns <code>Feature_ID</code> in <code>amo</code> and <code>ASVID</code> in <code>asv</code> are keys\".</p> <p>The function <code>left_join()</code> is one of four mutating (sensu <code>mutate()</code> where new columns are appended to the table) joins in <code>dplyr</code>. The others are:</p> <ul> <li><code>right_join()</code> Joins the table on the left to the table on the right (i.e., <code>left_join()</code> in the opposite direction).</li> <li><code>inner_join()</code> Adds columns for matching keys in both tables.</li> <li><code>full_join()</code> Adds columns for all keys in both tables.</li> </ul> <p>Practical use</p> <p>In my experience, <code>left_join()</code> is sufficient for most cases and is often the default choice for many users for its readable syntax (especially in chained pipes). However, there are some compelling cases for using <code>inner_join()</code> and <code>full_join()</code>.</p> <p>Many bioinformatics analyses are run in parallel (e.g., annotations against multiple databases or sequence curation using multiple software) to obtain as much information as possible about a given sequence. Essentially, these are identical sequences analysed in different ways.</p> <p>Let\u2019s assume that we are interested in well-characterised genes and pathways. Multiple analyses should produce concordant results for a subset of sequences thereby reducing the chance of false positives or hits with low probability of homology (e.g., a query sequence with homology to citrate synthase should be found regardless of searches against KEGG, NCBI RefSeq, or UniProt databases). In this scenario, using <code>inner_join()</code> will create a table that includes only those results that are consistent across analyses.</p> <p>Under different circumstances we might be interested in looking at all the outputs captured by multiple analyses. For example when looking for novel or less well-characterised genes, suspect some analyses may propagate database biases, or want to perform further analyses to eliminate redundancies based on home-grown criteria. Here, we are likely more interested in preventing false negatives. In this case use <code>full_join()</code> to preserve all data points for further curation.</p>"},{"location":"2.relational_data/#cleaning-up-taxon-and-summarising-data","title":"Cleaning up <code>Taxon</code> and summarising data","text":"<p>Next, lets use our skills from the previous lesson to only pick out \"Nitroso-\" from the Taxon column. Given that <code>ggplot2</code> requires data in \"long\" format, we will also pivot the data and create sums of the abundance values so we have an overview of the distribution of ammonia oxidisers.</p> <p>code</p> <pre><code>amo_summary &lt;- amo_abd %&gt;%\n  mutate(\n    Taxon = str_replace(Taxon, \".*(Nitroso[^;]+).*\", \"\\\\1\")\n  ) %&gt;%\n  pivot_longer(\n    cols = where(is.numeric), \n    names_to = \"sample\", \n    values_to = \"abundance\"\n  ) %&gt;%\n  group_by(sample, Taxon) %&gt;%\n  summarise(\n    total = sum(abundance, na.rm = TRUE)\n  )\n</code></pre>"},{"location":"2.relational_data/#joining-environmental-variables","title":"Joining environmental variables","text":"<p>Finally, we need to join environmental variables to our ammonia oxidiser abundance table. Then, we can plot the data using <code>ggplot2</code>.</p> <p>code</p> <pre><code>amo_summary &lt;- left_join(amo_summary, env, by = join_by(\"sample\"))\n\n# Plot using ggplot2\nggplot(amo_summary, aes(x = dnpoc, y = total, colour = Taxon)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F)\n</code></pre>"},{"location":"2.relational_data/#duplicated-keys","title":"Duplicated keys","text":"<p>The keys in the <code>asv</code> table are MD5 hash values generated directly based on the sequence data. A benefit of using this convention is that adding more sequences to the analysis involves very little remapping of sequence to IDs. Thanks to the way MD5 hash algorithm works, read counts from the new batch will be added to read counts from the previous batch if they were the same ASV, while new unique sequences will be appended to the table as new entries.</p> <p>However, what would happen if we tried to join data that had redundancies in their key naming conventions? In this case, the default behaviour of mutating joins is to output the Cartesian product of both data frames based on their keys. That is to say, all possible combinations of the keys will be used to generate the newly joined table.</p>"},{"location":"2.relational_data/#optional-filtering-joins","title":"(Optional) Filtering joins","text":"<p><code>dplyr</code> also has two filtering joins (sensu <code>filter()</code> where rows are removed/retained based on keys). These are:</p> <ul> <li><code>anti_join()</code> removes rows if keys in <code>x</code> match those in <code>y</code>.</li> <li><code>semi_join()</code> retains rows if keys in <code>x</code> match those in <code>y</code>.</li> </ul> <p>These are similar to piping a data frame to a <code>filter()</code>:</p> <p>code</p> <pre><code># anti_join(x, y)\nfilter(x, key %in% y$key)\n\n# semi_join(x, y)\nfilter(x, !(key %in% y$key))\n</code></pre> <p>Personally, I prefer using <code>filter()</code> for its flexibility and verbose syntax, and then perform joining operations when necessary.</p>"},{"location":"3.function_conditions/","title":"Functions and conditional statements","text":"<p>Learning objectives</p> <ul> <li>Identify parts of a function</li> <li>Write custom functions</li> <li>Use conditional statements to control outputs</li> <li>Refine functions to improve efficiency</li> <li>Access function source code from other packages</li> </ul>"},{"location":"3.function_conditions/#anatomy-of-functions","title":"Anatomy of functions","text":"<p>It's not a secret that for as long you've used R, you've been using functions. Here, we're going to make them! Functions are, like all things in R, a type of object (implications discussed in a later episode). They are the \"verbs\" of the R programming language where, instead of storing data, functions store code that defines how it behaves and operates on other objects.</p> <p>In an R script, functions are created and assigned using the following syntax:</p> <pre><code>f &lt;- function(&lt;arguments&gt;) {\n\n  &lt;do_something&gt;\n\n}\n</code></pre> <p>Lets break down the anatomy of the pseudocode:</p> <ul> <li><code>f</code> is the assigned name of the function.</li> <li><code>function()</code> initialises a function call, telling R that<ul> <li>We are writing a function</li> <li>Anything inside round brackets are arguments for the function</li> <li>Anything after the closing round bracket and on the same line is the code that defines the function</li> </ul> </li> <li><code>{   &lt;do_something&gt;   }</code> is the expression that defines how the function works</li> </ul> <p>Let's write a function to give some context as to what each part does.</p> \\[ f(x) = \\sqrt{2(x + 1)} \\] <p>The above is an equation for a function called \\(f\\) that takes a value \\(x\\) as input and outputs a value based on \\(x\\). We can translate that into R code:</p> <p>code</p> <pre><code>f1 &lt;- function(x) {\n  sqrt(2 * (x + 1))\n}\n</code></pre> <p>Here, we've \"wrapped\" an arithmetic operation in a function.</p> <p>Alternative syntax</p> <p>We can also use a shorthand for the <code>function()</code> call by replacing <code>function</code> with a <code>\\</code>:</p> <p>code</p> <pre><code>f1 &lt;- \\(x) sqrt(2 * (x + 1))\n</code></pre> <p>This syntax is useful for on-the-fly functions that can be applied using <code>map()</code> functions.</p> <p>Notice also that the function expression is on the same line as the function call. This syntax is acceptable for short expressions but not encouraged for longer ones.</p> <p>Infix functions</p> <p>Most objects we think of as functions are prefix functions, where arguments are placed after the function call. However, there are also infix functions, where arguments are placed on either side of the function call. For example:</p> <p>code</p> <pre><code>x &lt;- 2 + 3\n</code></pre> <p>Both <code>&lt;-</code> and <code>+</code> are infix functions for assignment and addition, respectively. All basic arithmetic (e.g., <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>), assignment (including <code>=</code>), logical (e.g., <code>|</code>, <code>&amp;</code>, <code>&amp;&amp;</code>, etc.), and relational (e.g., <code>&gt;</code>, <code>!=</code>, <code>==</code>, etc.) operators are primitive (i.e., foundational part of the language) infix functions.</p> <p>The pipe operator <code>%&gt;%</code> introduced by the <code>magrittr</code> package is a user-generated infix function. As this is a custom infix function, the <code>%</code> that surrounds the operator is necessary. It does not have to be a symbol or operator and can be represented by anything really, (e.g., <code>%in%</code>) even emojis!</p> <p>We can define our own infix function:</p> <p>code</p> <pre><code>`%&lt;I&gt;%` &lt;- function(lhs, rhs) {\n  # Get the minima from the left hand side and the maxima from the right hand side\n  c(min(lhs), max(rhs))\n}\n\nrnorm(10) %&lt;I&gt;% rnorm(200)\n</code></pre>"},{"location":"3.function_conditions/#control-flow-1-conditional-execution","title":"Control flow 1: Conditional execution","text":"<p>We will take a little detour from functions to learn about control flow. These are constructs that code for decision making (this episode) and repetition (next episode). Decision making/conditional execution helps to direct the \"flow\" of operations using Boolean logic (<code>TRUE</code>/<code>FALSE</code>). In R, conditional execution is coded using <code>if-else</code> statments:</p> <pre><code>if (&lt;predicate&gt;) {\n\n  &lt;expression_1&gt;\n\n} else {\n\n  &lt;expression_2&gt;\n\n}\n</code></pre> <p>The pseudocode above means <code>if</code> the <code>&lt;predicate&gt;</code> is (or returns a) <code>TRUE</code>, execute <code>&lt;expression_1&gt;</code>, otherwise execute <code>&lt;expression_2&gt;</code>.</p> <p>For example:</p> <p>code</p> <pre><code>if (x &lt; 10) {\n  print(\"x is less than 10\")\n} else {\n  print(\"x is more than 10\")\n}\n</code></pre> <p>Predicate and predicate functions</p> <p>A predicate is a statement or function that evaluation that returns one <code>TRUE</code> or <code>FALSE</code> value. For example, this is a predicate that evaluates if <code>x</code> is larger than 5:</p> <pre><code>x &gt; 5\n</code></pre> <p>If we set <code>x &lt;- 10</code>, the code will return a <code>TRUE</code>.</p> <p>R has many built-in predicate functions (i.e., functions that evaluate a statement or object and returns a <code>TRUE</code>/<code>FALSE</code>). They are often prefixed using <code>is.&lt;something&gt;()</code>. For example:</p> <p>code</p> <pre><code>x &lt;- 9\ntim &lt;- \"Tim is eating\"\ny &lt;- 1:6\n\n# Predicate function\nis.character(x)\nis.character(tim)\nis.character(y)\nis.numeric(y)\n</code></pre> <p>Notice that regardless of the length of the vector, the predicate functions return only one Boolean variable. This is important for conditional statements as it can only evaluate predicates and not a vector of Boolean variables.</p> <p>The <code>else</code> statement is optional (and often unnecessary). The expression will not be evaluated if the predicate returns <code>FALSE</code>.</p> <p>code</p> <pre><code># Predicate returns TRUE, expression is evaluated\nif (x &lt; 10) {\n  print(\"x is less than 10\")\n}\n\n# Predicate returns FALSE, expression is not evaluated\nif (x &gt; 10) {\n  print(\"x is greater than 10\")\n}\n</code></pre> <p>For complex predicate evaluations (e.g., intermediate thresholds) or evaluations that lead to different paths (e.g., hierarchical decision making), we can chain and/or nest multiple statments together:</p> <pre><code>if (&lt;predicate_1&gt;) {\n\n  &lt;expression_1&gt;\n\n} else if (&lt;predicate_2&gt;) { # Chained statements: an if-else after an if-else\n\n  if (&lt;predicate_2a&gt;) { # Nested statements: an if-else inside and if-else\n\n    &lt;expression_2a&gt;\n\n  } else {\n\n    &lt;expression_2b&gt;\n\n  }\n\n} else {\n\n  &lt;expression_3&gt;\n\n}\n</code></pre>"},{"location":"3.function_conditions/#vectorised-conditional-execution","title":"Vectorised conditional execution","text":"<p>The above conditional statements only operate when there is only one Boolean value. What if we wanted to generate different outcomes depending on the elements in a vector? In this case, we can use these two functions depending on how many outcomes we need:</p> Binary outcomesMultiple conditions and outcomes <p>If we needed one outcome each for <code>TRUE</code> or <code>FALSE</code>, we can use <code>if_else()</code>, a vectorised if-else statement that evaluates a Boolean vector (or a condition that returns one) and produces outcomes depending on whether each element is a TRUE or FALSE. For example:</p> <p>code</p> <pre><code>fridge &lt;- c(\"banana\", \"apple\", \"kale\", \"durian\", \"pear\", \"spinach\")\n\nif_else(\n  fridge %in% fruit,\n  str_c(str_to_upper(fridge), \" is a fruit\"),\n  str_c(fridge, \" is not a fruit\")\n)\n</code></pre> <p>Base R equivalent: <code>ifelse()</code></p> <p>Base R also has a similar implementation: <code>ifelse()</code>. However, <code>dplyr::if_else()</code> strictly preserves the data type (e.g., the output will not be coerced to a character if the input is a factor) and can evaluate missing values; <code>base::ifelse()</code> does not enforce data type preservation and returns missing values when it encounters them.</p> <p>If we required different outcomes based on multiple, hierarchical conditions, we can use <code>case_when()</code>, a general vectorised if-else analogous to chained if-else statements. For example:</p> <p>code</p> <pre><code>fridge &lt;- c(\"cheese\", fridge, \"milk\")\ndairy &lt;- c(\"cheese\", \"yoghurt\", \"butter\", \"milk\")\nvegetable &lt;- c(\"beetroot\", \"beans\", \"kale\")\n\ncase_when(\n  fridge %in% fruit ~ str_c(fridge, \" is a fruit\"),\n  fridge %in% vegetable ~ str_c(fridge, \" is a vegetable\"),\n  fridge %in% dairy ~ str_c(fridge, \" is a dairy product\"),\n  .default = str_glue(\"You don't even like {fridge}!\")\n)\n</code></pre>"},{"location":"3.function_conditions/#alpha-diversity","title":"Alpha diversity","text":"<p>Time to apply all of the above to write a function! For this section, we will be writing a function that calculates the \\(\\alpha\\)-diversity (a measure local diversity) of a sample. The simplest measure is richness (often denoted \\(q\\) or \\(S\\)), which represents the number of species/taxonomic units. While richness is simple and intuitive to understand, it is heavily biased by rare members of the sample community. This is always the case with microbiome data. Other \\(\\alpha\\)-diversity indices were constructed to account for the uneven distribution of taxonomic units by incorporating relative abundance information. Popular options are:</p> <p>Shannon's index, \\(H\\)</p> \\[ H = -\\sum_{i=1}^{q} p_{i} \\log p_{i} \\] <p>Where \\(p_{i}\\) is the proportion (or relative frequency) of the \\(i^{th}\\) organism relative to the sample total.</p> <p>Simpson's diversity, \\(D\\)</p> \\[ D = \\frac{1}{\\lambda} \\] <p>Where the \\(\\lambda\\) is Simpson's concentration index, defined as</p> \\[ \\lambda = \\sum_{i=1}^{q} p_{i}^2 \\] <p>For the mathematically inclined, you might observe that these indices:</p> <ul> <li>Gives more weight to abundant taxa (\\(D\\) moreso than \\(H\\))</li> <li>Reach their maxima when all taxa are exactly evenly distributed in the sample</li> <li>Reach their minima when one taxa that dominate</li> <li>Are not on the same scale (\\(H\\) is on a log scale, \\(q\\) and \\(D\\) are on the \"number of taxonomic units\" scale)</li> </ul> <p>For now, it is enough to know that they are mathematical formulas that we are going to translate into code.</p>"},{"location":"3.function_conditions/#starting-small-richness","title":"Starting small: richness","text":"<p>We will start by writing a function to calculate richness (the number of taxonomic units in the sample). Applied in R, this means the number of non-zero elements in a numeric vector.</p> <p>code</p> <pre><code># Test vector\ntest_vector &lt;- asv$AS1A3\n\n# Richness\ncalc_alpha_diversity &lt;- function(x) {\n  sum(x &gt; 0)\n}\n\n# Test function\ncalc_alpha_diversity(test_vector)\n</code></pre>"},{"location":"3.function_conditions/#adding-arguments-shannons-and-simpsonss-diversity","title":"Adding arguments: Shannon's and Simpsons's diversity","text":"<p>Next, lets add the other indices to be part of the function. We will use conditional execution to control output depending on the index is requested.</p> <p>code</p> <pre><code>calc_alpha_diversity &lt;- function(x, index) {\n\n  # Richness\n  q &lt;- sum(x &gt; 0)\n\n  # Shannon's index\n  p &lt;- x / sum(x)\n  H &lt;- - sum(p * log(p))\n\n  # Simpson's diversity\n  p &lt;- x / sum(x)\n  D &lt;- 1 / sum(p ^ 2)\n\n  if (index == \"richness\") {\n    return(q)\n  } else if (index == \"shannon\") {\n    return(H)\n  } else if (index == \"simpson\") {\n    return(D)\n  }\n\n}\n\n# Test that it works\ncalc_alpha_diversity(test_vector, index = \"richness\")\ncalc_alpha_diversity(test_vector, index = \"shannon\")\ncalc_alpha_diversity(test_vector, index = \"simpson\")\n</code></pre> <p>Here, we're calculating all possible outcomes, then returning values based on the index requested.</p> <p><code>return()</code></p> <p>The <code>return()</code> function returns/outputs the value specified in it and stops evaluation of any code below it. In <code>calc_alpha_diversity()</code>, if the index requested is richness, and subsequent code to calculate the other indices are not evaluated. In the case where a function code has no <code>return()</code>, the function will output the last evaluation.</p> <p>The function we wrote works for \\(q\\) and \\(D\\), but not for \\(H\\). Lets revise that.</p>"},{"location":"3.function_conditions/#troubleshooting-and-refining-code","title":"Troubleshooting and refining code","text":"<p>Lets troubleshoot our function first. Why did the code not work for Shannon's index?</p> <p>A little math</p> <p><code>log(0)</code> is undefined. However, in R, it is given the value <code>-Inf</code> for bases &gt; 0 and <code>Inf</code> for bases \\&lt; 0. If any <code>p</code> is 0, the operation <code>0 * log(0)</code> will return <code>NaN</code> (Not a Number). Therefore, the summation of that will return <code>NA</code>.</p> <p>We will also consider other areas for refinement:</p> Repeatedly assigning <code>p</code>Assigning <code>H</code> and <code>D</code>Chained if-elseChoice of logarithm base <p>Relative frequency <code>p</code> is not required for richness calculations and is shared for calculations of Shannon's and Simpson's indices.</p> <p>Assigning of \\(H\\) and \\(D\\) helps us, the human, to differentiate the indices in mathematical notation. However, R does not care which name is used. The decision to return either index is already managed by conditional statements.</p> <p>Chained statements are not necessary here. Conditional execution (and <code>return()</code>) should occur earlier to prevent unnecessary code evaluation.</p> <p>Some people may have a reason to use another base for the <code>log()</code> function used in calculating Shannon's index. While the natural logarithm (log in base \\(e\\)) is often the default choice, there should be an option to change that if desired. To set a default value in a custom function, add the desired value to the argument within the function call (see below).</p> <p>With that, lets revise the function:</p> <p>code</p> <pre><code>calc_alpha_diversity &lt;- function(\n  x, index, \n  log.base = exp(1) # Provide natural logarithm as default\n) {\n\n  # Richness\n  if (index == \"richness\") {\n    return(sum(x &gt; 0))\n  }\n\n  # Relative frequency\n  p &lt;- x / sum(x)\n\n  # Shannon's index\n  if (index == \"shannon\") {\n    H &lt;- - sum(p * log(p, base = log.base), # Provide option for changing log base\n               na.rm = TRUE)                # Removes NA prior to summation\n  }\n\n  if (index == \"simpson\") {\n    H &lt;- 1 / sum(p ^ 2)\n  }\n\n  H\n\n}\n\n# Test the function\ncalc_alpha_diversity(test_vector, index = \"richness\")\ncalc_alpha_diversity(test_vector, index = \"shannon\")\ncalc_alpha_diversity(test_vector, index = \"simpson\")\n</code></pre> <p>It works! Note that without a return, the function simply outputs the last evaluted code (in this case, this is the value <code>H</code>).</p>"},{"location":"3.function_conditions/#defending-our-function","title":"Defending our function","text":"<p>What would happen if we used our functions with non-ideal inputs? For this, we will take hints from the concepts in defensive programming. Defensive programming is more about design choices than cybersecurity, where we approach designing code that:</p> <ul> <li>Has predictable outputs, regardless of inputs</li> <li>\"Fails fast\", thus saving time and computational resources</li> <li>Reads well, so others can audit the code and understand what each line is doing</li> </ul> <p>Can you identify some scenarios where our function might not do so well?</p> Pitfalls and food for thought Missing valuesNegative numbersNon-numeric data <p>What does <code>NA</code> mean?</p> <p>Lets consider how these tables were constructed. They were originally sequences that were processed by a software and then reads were counted for each taxonomic unit. If a taxonomic unit was not found in a particular sample read file, the software would have returned a 0. Therefore, <code>NA</code> could not have arisen from processing this particular batch.</p> <p>However, we can imagine a scenario where <code>NA</code> could be introduced from a joining step during data cleaning or pre-processing. This can happen when processing samples by batches.</p> <p>Critically, regardless of the reasons for missingness, is a missing count functionally equivalent to 0?</p> <p>What is the effect of <code>NA</code>?</p> <p>We used <code>sum()</code> quite often in the code. Furthermore, we even added <code>na.rm = TRUE</code> in the expression used to calculate Shannon's index to account for the <code>NA</code> introduced when evaluating \\(0 \\log 0\\). Would we handle <code>NA</code> in all parts where <code>sum()</code> is used in this way?</p> <p>Do negative numbers make sense?</p> <p>Remember that this is a table of counts. What do negative numbers mean in that context? Does it make sense?</p> <p>How does the function handle negative numbers?</p> <p>\\(q\\) would ignore negative numbers. If the negative counts were an oversight or accidental, it would result in an undercount of richness.</p> <p>\\(H\\) would not produce a numeric output (the log of a negative number is undefined).</p> <p>\\(D\\) would be an underestimate (the sum would be an undercount of the whole).</p> <p>Does non-numeric data make sense for this function?</p> <p>These indices are meant for count data (i.e., numeric data). Suppose that human error led to the mis-typing of inputs, would we want to coerce the data into numeric type using <code>as.numeric()</code>? Are we guessing the intentions and errors of the user? Coercing characters that are not numbers will cause <code>as.numeric()</code> to return <code>NA</code>.</p> <p>Is it fit for purpose?</p> <p>We established that the function is meant to provide information only on numeric data. Albeit being a small function that runs quickly, we can imagine a scenario where we might either (1) write a function that has many complex steps or (2) run a substantially larger data set through the diversity function. In either scenario, would we want to spend time and computational resources on non-target inputs?</p> <p>Let's test our function through these non-ideal scenarios. Create vectors with non-ideal properties and try them on the current version of the function.</p> <p>code</p> <pre><code># Randomly introduce NA\ntest_missing &lt;- test_vector\ntest_missing[sample.int(length(test_missing), 5)] &lt;- NA\n\n# Randomly introduce negative numbers\ntest_negative &lt;- ifelse(\n  test_vector &gt; 0 &amp; runif(length(test_vector)) &gt; 0.95,\n  -test_vector, test_vector\n)\n\n# Coerce to character\ntest_character &lt;- as.character(test_vector)\n</code></pre> <p>Random replacements</p> <p>The code above uses two ways to generate replacements at random. In line 3, a random sampler <code>sample.int()</code> was used to indicate 5 random indices to which we wanted to inject <code>NA</code>. In lines 6-9, a conditional statement was constructed to replace positive, non-zero values, picked based on a numeric (double) vector picked from a uniform distribution, with negative equivalents.</p> <p>With those pitfalls in mind, we will need to make some design decisions and implement them in code. Here are my choices:</p> Remove missing valuesError on negative numbers and non-numeric data <p>Functionally, they are the same as not observing a taxa in a sample (equivalent to 0). Warn the user when this happens but continue with evaluating input.</p> <p>These data types are not intended for use with diversity indices here. Lets not spend computing time and resources on them. To achieve this, we will \"error out\" of the function where if negative numbers and non-numeric data are encountered, the function will stop further computation and produce an error message.</p> <p>Feel free to implement your own design choices by modifying the code below!</p> <p>code</p> <pre><code>calc_alpha_diversity &lt;- function(\n  x, index, \n  log.base = exp(1) # Natural logarithm as default\n) {\n\n  # Warn for missing values\n  if (anyNA(x)) {\n    warning(\"x contains NAs which are removed prior to calculations.\")\n    x &lt;- x[!is.na(x)]\n  }\n\n  # Error on non-numeric and negative data\n  if (!is.numeric(x) | any(x &lt; 0)) {\n    stop(\"x must be positive numeric values!\")\n  }\n\n  # Richness\n  if (index == \"richness\") {\n    return(sum(x &gt; 0))\n  }\n\n  # Relative frequency\n  p &lt;- x / sum(x)\n\n  # Shannon's index\n  if (index == \"shannon\") {\n    H &lt;- - sum(p * log(p, base = log.base), # Provide option for changing log base\n               na.rm = TRUE)                # Removes NA from 0 * log(0)\n  }\n\n  if (index == \"simpson\") {\n    H &lt;- 1 / sum(p ^ 2)\n  }\n\n  H\n\n}\n</code></pre> <p>Try the new function on the test vectors:</p> <p>code</p> <pre><code>calc_alpha_diversity(test_missing, index = \"shannon\")\ncalc_alpha_diversity(test_negative, index = \"simpson\")\ncalc_alpha_diversity(test_character, index = \"richness\")\n</code></pre>"},{"location":"3.function_conditions/#improve-function-design","title":"Improve function design","text":""},{"location":"3.function_conditions/#read-other-functions","title":"Read other functions","text":"<p>One of the best ways to improve how we write functions is to read other people's functions! Not only do we get to see how others design their functions, we can also learn new and useful functions along the way!</p> <p>We can access the source code of a function using <code>getAnywhere()</code>:</p> <p>code</p> <pre><code>getAnywhere(lm)\n</code></pre> <p>Some functions may not have directly accessible source code. For example:</p> <p>code</p> <pre><code>getAnywhere(anova)\n</code></pre> <p>Notice that the body of the function consists only of <code>UseMethod(\"anova\")</code>. This is because <code>anova()</code> can take many different kinds of inputs and run different code depending on the input (i.e., it has different <code>methods()</code> for the same function). We can see what those methods are and inspect the source code for any of them:</p> <p>code</p> <pre><code># What methods are available for anova?\nmethods(anova)\n\n# Inspect anova.lm\ngetAnywhere(anova.lm)\n</code></pre> <p>In some cases, a function could have the same name but are implemented differently because they are part of a different package. For example:</p> <p>code</p> <pre><code>getAnywhere(filter)\n</code></pre> <p>To access the <code>filter()</code> source code from the <code>stats</code> package, we need pick one by its index:</p> <p>code</p> <pre><code># Get source code for filter in stats package\ngetAnywhere(filter)[2]\n</code></pre>"},{"location":"3.function_conditions/#return-early","title":"<code>return()</code> early","text":"<p>Use <code>return()</code> early in the source code especially if other arguments will lead to more time consuming evaluations. This prevents wasteful use of time and resources to compute variables that will not even be used as outputs!</p>"},{"location":"3.function_conditions/#switch-evaluations-depending-on-argument-input","title":"<code>switch()</code> evaluations depending on argument input","text":"<p><code>swtich()</code> is a useful function that evaluates different expressions depending on the argument inputs. It can also be given a default output when no arguments match those specified which can be coded to output a warning or message.</p> <p>code</p> <pre><code>calc_alpha_diversity &lt;- function(x, index, log.base = exp(1)) {\n\n  # Warn for missing values\n  if (anyNA(x)) {\n    warning(\"x contains NAs which are removed prior to calculations.\")\n    x &lt;- x[!is.na(x)]\n  }\n\n  # Error on non-numeric and negative data\n  if (!is.numeric(x) | any(x &lt; 0)) {\n    stop(\"x must be positive numeric values!\")\n  }\n\n  # Richness\n  if (index == \"richness\") {\n    return(sum(x &gt; 0))\n  }\n\n  # Relative frequency\n  p &lt;- x / sum(x)\n\n  # Indices\n  switch(\n    index, # Value to be matched against in the list below\n    shannon = - sum(p * log(p, base = log.base), na.rm = TRUE),\n    simpson = 1 / sum(p ^ 2),\n    warning(\"Unknown index. Should be one of 'richness', 'shannon', or 'simpson'.\") # Default output if unmatched\n  )\n\n}\n</code></pre>"},{"location":"3.function_conditions/#think-of-the-users","title":"Think of the users","text":"<p>Who is the target user for your functions?</p> <p>If it's just you, do whatever you want! However, the fact that you are assigning code to functions implies that you probably want to reuse it for other things/projects or even share them with your colleagues/the world!</p> <p>If you intend to share your functions with other people/computers/robots, it is important to think about dependencies. Other people will have different set-ups. Thus, it is advisable to write functions using other functions in base R. If you must use other packages, make sure to advise and install early in the script.</p> <p>While we write functions to be as predictable, elegant, and concise as possible, there are situations where we want to add flexibility. Keep in mind that flexible functions often involve more code. To achieve this, we can write functions that call other custom functions. This is called modularisation. Modular code also helps with troubleshooting, readability, and benchmarking as pieces of it can be tested. This is especially important for production-level code.</p> <p>What stage are you at?</p> <p>Assuming you are writing functions for your analyses, the syntax of your functions are likely geared towards your specific needs at a particular given stage. At the exploratory analysis stage, functions are written with flexbility in mind, perhaps even a little messy or less efficient that it could be.</p> <p>As you progress to production level code and/or need to use complex processes (permutation tests, sub-sampling, Bayesian methods, etc.), you're likely going to need to think about:</p> <p> Code safety Predictable and non-redundant inputs and outpus, and preventing ad inifinitum runs</p> <p> Efficiency Use benchmarked algorithms, reduce/remove side effects, parallel processing, reduce overheads</p> <p> Troubleshooting Informative errors and warnings, modularised functions</p> <p> Readability Commented code, verbose syntax, appropriate assignments (as opposed to long piped processes)</p>"},{"location":"3.function_conditions/#break-your-own-code","title":"Break your own code!","text":"<p>The best way to learn from your own functions? Break them! How would it handle nonsensical inputs? What are its limits? How much data is too much data? Is it taking seconds, minutes, days to run? We can learn so much about how the function (and R in general) works when we stress test them.</p>"},{"location":"4.iterations/","title":"Iterations","text":"<p>Learning objectives</p> <ul> <li>Construct <code>for</code> loops</li> <li>Apply functions iteratively using <code>map_*()</code></li> <li>Make \"good enough\" design choices when iterating</li> </ul>"},{"location":"4.iterations/#say-again","title":"Say again?","text":"<p>Here, we leave behind copy-pasting and really start to accelerate workflows. To iterate means to do something repetitively on different objects. All programming languages have at least one implementation of iterations (arguably the main reason to do anything on a computer). In R, this is done in one of two ways:</p> <p><code>for</code> loop</p> <p>This is a ubiquitous method across many programming languages with slight syntax differences between languages.</p> <p>Applying functions using <code>map()</code></p> <p>\"Applies\" a function along a vector/list with more concise code. In other words, the function supplied is recycled for every element in the vector/list. The iteration is happening in the background.</p> <p>This episode will cover how to use both methods via a pairwise correlation analysis. The goal of the analysis is to determine what environmental factors might be correlated with sequence variant, and consequently, taxa, distribution. Finally, we will discuss some design choices with regards to iterations based on data properties.</p>"},{"location":"4.iterations/#for-loops","title":"<code>for</code> loops","text":"<p>A <code>for</code> loop is the most common way to iterate through objects. All <code>for</code> loops look like the following:</p> <pre><code>for (&lt;iterator&gt; in &lt;vector&gt;) {\n\n  &lt;expressions&gt;\n\n}\n</code></pre> <p>Here is what each part of the pseudocode above means:</p> Code Description <code>for</code> Initialises the loop. <code>&lt;iterator&gt;</code> A variable that represents each element of the object being iterated on <code>&lt;vector&gt;</code> An object to iterate <code>&lt;expressions&gt;</code> Lines of code that does something per iteration <p>Here is a simple working example:</p> <p>code</p> <pre><code>for (i in 1:10) {\n  print(i * 2)\n}\n</code></pre> <p>The above example iterated through numbers <code>1:10</code>, where each number was represented by <code>i</code> (i.e., <code>for (i in 1:10)</code>), then multiplied each number by <code>2</code>, then printed the result on the console (i.e., <code>{print(i * 2)}</code>).</p> <p>The syntax of a <code>for</code> loop is remarkably flexible and can be used in a variety of scenarios. Powerful as it is, there are a few best practices that we need to be mindful of. We will get to them in our practical exercise section below.</p>"},{"location":"4.iterations/#handling-outputs","title":"Handling outputs","text":"<p>Consider the following loop:</p> <p>code</p> <pre><code>for (i in 1:10) {\n  Ni &lt;- i * 2\n}\n</code></pre> <p>We constructed a loop to multiply each number by 2, but we only have 1 number as a result. This is because we assigned <code>Ni</code> with a new value with each iteration. To overcome this, we need to prepare an output vector that can store the result of each iteration.</p> <p>code</p> <pre><code>Ni &lt;- numeric(10) # Produces a vector of 0s\n\nfor (i in 1:10) {\n  Ni[i] &lt;- i * 2\n}\n</code></pre> <p>Thy shall not grow vectors</p> <p>It is possible to create an empty vector and then growing it by appending values from a <code>for</code> loop.</p> <p>code</p> <pre><code>Ni &lt;- c()\n\nfor (i in 1:10) {\n  Ni &lt;- c(Ni, i * 2)\n}\n</code></pre> <p>However, this kind of construct is not recommended for most use cases. If an iteration generates large number of elements, there might not be sufficient RAM to hold it and the entire R session will stall and terminate.</p> <p><code>while</code> loops</p> <p>As seen above, <code>for</code> loops require a defined range for iteration. There are scenarios where we want an iteration to run for as long as required until some condition is reached. In this case, we can use the <code>while</code> loop. </p> <pre><code>while(&lt;predicate&gt;) {\n\n  &lt;statement&gt;\n\n}\n</code></pre> <p>As long as the expression returns a <code>TRUE</code>, the statement will continue to run. This is common in numerical optimisation where procedures are required for model fitting.</p> <p>The following code block illustrates how <code>while</code> loops are used in an iterative process to obtain small number solutions to the Collatz conjecture.</p> <p>code</p> <pre><code>collatz &lt;- function(n) {\n  # Output\n  PATH &lt;- numeric(10)\n\n  # Initiate index\n  i &lt;- 1\n\n  # Expand output stepwise\n  if (i == length(PATH)) {\n    PATH &lt;- c(PATH, numeric(10))\n  }\n\n  # Exit\n  if (n == 1) return(0)\n\n  # Test\n  while (n != 1) {\n    n &lt;- if_else(n %% 2 == 0, n / 2, (3 * n) + 1)\n    PATH[i] &lt;- n\n    i &lt;- i + 1\n  }\n\n  PATH[PATH &gt; 0]\n}\n\n# Solutions for numbers 2 to 50\ncollatz_res &lt;- vector(mode = \"list\", length = 100 - 1)\nfor (numbers in 2:100) {\n  collatz_res[[numbers - 1]] &lt;- collatz(numbers)\n  names(collatz_res)[numbers - 1] &lt;- numbers\n}\n</code></pre> <p>In lines 19 to 23, we applied a <code>while</code> loop to ensure that <code>collatz()</code> continues to be applied until <code>x == 1</code>, which marks the end of the loop. </p>"},{"location":"4.iterations/#the-map-family","title":"The <code>map()</code> family","text":"<p>While <code>for</code> loops are flexible and powerful, writing them can sometimes result in long expressions. This hampers others (including future self) from auditing/reading the code. To help alleviate some of this, we can use the <code>purrr::map()</code> family of functions. These functions aim to make code more function-oriented and makes code more concise. The table below shows some commonly used ones from both families:</p> Function Description <code>map()</code> Iterate over a vector. <code>map2()</code> Iterate over 2 vectors. <code>pmap()</code> Iterate over multiple vectors. <p>The base forms above always return a list regardless of coercion within the \"mapped\" function. However, each function also have modifiers that can output different modes of data:</p> Modifier Return values <code>_int()</code> Integer (numeric whole numbers) <code>_dbl()</code> Double (numeric with decimals) <code>_chr()</code> Character <code>_lgl()</code> Boolean (<code>TRUE</code> or <code>FALSE</code>) <p>These functions are designed to produce predictable outputs (or, as written in the help pages, \"die trying\"). They all have the following syntax:</p> <pre><code># One vector\nmap(vector, &lt;function,character,integer&gt;)\n\n# Two vectors\nmap2(vector_1, vector_2, &lt;function,character,integer&gt;)\n\n# List of multiple vectors\npmap(list(vector_1, vector_2, vector_3, ...), &lt;function,character,integer&gt;)\n</code></pre> <p>If a character or integer is supplied in place of a function, it will extract the component by name or position, respectively.</p> <p>We'll make some examples to get a grasp of what <code>map()</code> does:</p> <p>code</p> <pre><code># Something to iterate\nb &lt;- list(\n  some_characters = fruit[1:10],\n  some_logical = sample(c(TRUE, FALSE), 10, replace = TRUE),\n  some_integers = rpois(10, 2),\n  some_doubles = rcauchy(10)\n)\n\nmap(b, \\(x) paste(x, collapse = \" \"))\n</code></pre> <p>We wrote a simple expression that concatenates elements of vectors in list <code>b</code> and prints them out as one string per list element. Here's another example that uses a variation of <code>map_*()</code> to calculate the coefficient of variation among biological replicates:</p> <p>code</p> <pre><code>example_pmap &lt;- pmap_dbl(\n  list(asv$AS1A1, asv$AS1A2, asv$AS1A3),\n  \\(a, b, c) {\n    values &lt;- c(a, b, c)\n    sd(values) / mean(values)\n  }\n)\n\nstr(example_pmap)\n</code></pre> <p>Notice that it is a numeric vector instead of a list as we used the <code>_dbl</code> modifier. If we tried to use a different modifier that does not match the output mode, it will error out and will not produce an output.</p> <p>Try different modifiers and see what are the outcomes. Include coercion inside the function and see if you can produce different results.</p> <p>Sidestepping for side effects</p> <p>The above <code>map_*()</code> family of functions all produce an output per element. However, if your primary interest is to generate side effects (i.e., plots, console outputs via <code>cat()</code>, saving files, etc.), it is better to use <code>purrr</code>'s family of functions: <code>walk()</code>. Like <code>map_*()</code>, it comes in variations for two (<code>walk2()</code>) or more (<code>pwalk()</code>) vectors. Compare the following outputs:</p> <pre><code>fn_glue &lt;- \\(x, nm) {\n  str_glue(\"Vector named {nm} is a {class(x)}\")\n}\n\nmap2(b, names(b), \\(x, nm) cat(fn_glue(x, nm), \"\\n\"))\nwalk2(b, names(b), \\(x, nm) cat(fn_glue(x, nm), \"\\n\"))\n</code></pre> <p>Let's compare and contrast use cases of <code>for</code> and <code>map()</code> using the exercise below.</p>"},{"location":"4.iterations/#pairwise-correlation-analysis","title":"Pairwise correlation analysis","text":"<p>Community data is inherently multivariate. These are counts of organisms across different habitats/environments. A useful piece of information is how some organisms are distributed depending on the characteristics of their habitat, e.g., nutrient supply, sediment permeability, salinity, elevation, etc. Here, correlation analysis can help us understand if some of these characteristics underlie an organism's distribution. At a higher level, large correlation analyses can help identify some conditions that are more suited to groups of organisms (correlations can be a precondition of cluster analysis). From there, other analyses or more robust sampling designs can be used to test the validity of our observations.</p> <p>Before delving into the analyses, it is important to filter out data that may not be important. This means removing ASV that are only sparsely present. For this exercise, we will retain ASVs that are present in more than half of all samples. We will also convert the data frames of <code>asv</code> and <code>env</code> into matrices for ease of processing.</p> <p>code</p> <pre><code># Filter data\nretain_threshold &lt;- nrow(env) / 2\nasv_retained &lt;- rowSums(ifelse(asv[, -1] &gt; 0, 1, 0)) &gt; retain_threshold\nasv_subset &lt;- asv[asv_retained, ]\n\n# Convert to matrices\nasv_subset_matrix &lt;- as.matrix(asv_subset[, -1])\nrownames(asv_subset_matrix) &lt;- asv_subset$ASVID\n\nenv_matrix &lt;- as.matrix(env[, -1])\nrownames(env_matrix) &lt;- env$sample\n</code></pre>"},{"location":"4.iterations/#table-to-table-using-nested-for-loops","title":"Table-to-table using nested <code>for</code> loops","text":"<p>In this method, we will:</p> <ol> <li>Create matrices for storing correlation coefficients and their P-values.</li> <li>Use nested <code>for</code> loops to iterate through the rows of <code>asv</code> and columns of <code>env</code>.</li> <li>Calculate correlation coefficients and significance per iteration</li> <li>Assign the values to relevant matrices</li> </ol> <p>Pre-assign outputs</p> <p>code</p> <pre><code># Assign output\nrho_matrix &lt;- matrix(\n  nrow = nrow(asv_subset_matrix),\n  ncol = ncol(env_matrix),\n  dimnames = list(\n    asv_subset$ASVID,\n    colnames(env)[-1]\n  )\n)\np_matrix &lt;- rho_matrix\n\nstr(rho_matrix)\nstr(p_matrix)\n</code></pre> <p>Run correlation analyses via nested loop</p> <p>code</p> <pre><code># The outer loop that loops through ASVs (rows)\nfor (i in 1:nrow(asv_subset_matrix)) {\n\n  abundance &lt;- as.numeric(asv_subset_matrix[i, ])\n\n  # Nested loop\n  for (j in 1:ncol(env_matrix)) { # Looping over environmental measures (columns)\n\n    env_value &lt;- as.numeric(env_matrix[, j])\n\n    # Correlation tests\n    test &lt;- cor.test(abundance, env_value)\n\n    # Assign values to output matrices\n    rho_matrix[i, j] &lt;- test$estimate\n    p_matrix[i, j] &lt;- test$p.value\n\n  }\n\n  # Print some output\n  if (i %% 50 == 0) cat(str_glue(\"{i} ASVs done\"), \"\\n\")\n\n  if (i == nrow(asv_subset_matrix)) cat(\"Completed!\\n\")\n\n}\n</code></pre> <p>Code breakdown</p> Code Description <code>for (i in 1:nrow(asv_subset_matrix)) {</code> Initiates the outer <code>for</code> loop to iterate over the rows of <code>asv_subset_matrix</code> <code>abundance &lt;- as.numeric(asv_subset_matrix[i, ])</code> Coerces the i<sup>th</sup> row of <code>asv_subset_matrix</code> into a numeric vector, then assigns it to an object called <code>abundance</code>. This object is temporary and is over-written with each iteration. <code>for (j in 1:ncol(env_matrix)) {</code> Initiates the inner loop to iterate over the columns of <code>env_matrix</code> <code>env_value &lt;- as.numeric(env_matrix[, j])</code> Like for <code>abundance</code>, coerces the j<sup>th</sup> column of <code>env_matrix</code> into a numeric vector, then assigns it to <code>env_value</code>, a temporary object. <code>test &lt;- cor.test(abundance, env_value)</code> This is the workhorse function that calculates the Pearson's correlation coefficient and performs hypothesis testing (i.e. is the coefficient different from 0). <code>rho_matrix[i, j] &lt;- test$estimate</code><code>p_matrix[i, j] &lt;- test$p.value</code> Assigns the correlation coefficient and P-value to it's relevant position in the results matrices <code>rho_matrix</code> and <code>p_matrix</code>. <code>if (i %% 50 == 0) cat(str_glue(\"{i} ASVs done\"), \"\\n\")</code><code>if (i == nrow(asv_subset_matrix)) cat(\"Completed!\\n\")</code> Prints some informative outputs for us to track it's progress at intervals of every 50 ASVs (the outer loop) until it is complete. <p>Check output</p> <p>code</p> <pre><code>head(rho_matrix)\nhead(p_matrix)\n</code></pre> <p><code>cor.test()</code></p> <p>Find out what <code>cor.test()</code> returns by reading the help page</p>"},{"location":"4.iterations/#cartesian-product-using-expandgrid-and-map2","title":"Cartesian product using <code>expand.grid()</code> and <code>map2()</code>","text":"<p>In the above method, we generated two matrices that stored different, but related, information. However, we could also create a single table that contained both coefficient and P-value. To achieve this, we will assign all outputs that come from <code>cor.test()</code> into one table that consists of all possible combinations between <code>colnames(env_matrix)</code> and <code>rownames(asv_subset_matrix)</code> (i.e., the Cartesian product).</p> <p>Pre-assign output</p> <p>code</p> <pre><code>corr_results &lt;- expand.grid(\n  \"ASVID\" = rownames(asv_subset_matrix),\n  \"env_var\" = colnames(env_matrix)\n)\n\nhead(corr_results)\n</code></pre> <p>Iterate through outputs</p> <p>For this, we use the <code>tidyverse</code> dialect to perform the entirety of the analysis. We will use <code>map2()</code> to iterate over both columns simultaneously to <code>mutate()</code> columns that will store the results.</p> <p>code</p> <pre><code>corr_results &lt;- as_tibble(corr_results) %&gt;% \n  mutate(\n    \"cor_test\" = map2(ASVID, env_var, \\(i, j) {\n      abundance &lt;- asv_subset_matrix[rownames(asv_subset_matrix) == i, ]\n      env_value &lt;- env_matrix[, colnames(env_matrix) == j]\n      cor.test(abundance, env_value)\n    }),\n    \"rho\" = map_dbl(cor_test, \"estimate\"),\n    \"p\" = map_dbl(cor_test, \"p.value\")\n  )\n</code></pre> <p>Code breakdown</p> <p>Specifically, we will look at the <code>map2()</code> and <code>map_dbl()</code> calls:</p> Code Description <code>map2(ASVID, env_var, \\(x, y) {&lt;expression&gt;})</code> Here, <code>map2()</code> takes in two vectors, namely <code>ASVID</code> and <code>env_var</code> and iterates through every element in each, in order of position. The last argument is the function that will actually process elements of the vectors. <code>map2()</code> will only take functions that takes 2 arguments as inputs (i.e., <code>i</code> and <code>j</code>). In here, I've used an anonymous function as we do not need to keep this in our environment. <code>map_dbl(cor_test, \"estimate\")</code> After performing the analysis, we need to extract the relevant information. Here, <code>map_dbl()</code> is a strict and specific version of <code>map()</code> where it will only accept and output data with the mode <code>double</code>, meaning numerics with decimals or floating points. To indicate what we wanted to extract, we have given it a string that indicates the name of the vector we wish to take. You can also provide a function if you'd like, as long as the output of the function is strictly a <code>double</code>."},{"location":"4.iterations/#rules-to-iterate-by","title":"Rules to iterate by","text":"<p>As shown, <code>for</code> loops and <code>map()</code> are incredibly powerful tools. However, there are some best practices that should be adhered to in most cases to ensure code runs and exits safely. The sections below list some of them and scenarios where exceptions may arise.</p>"},{"location":"4.iterations/#iterate-indices","title":"Iterate indices","text":"<p>Iterating over indices of a vector, instead of directly iterating on the vector, allows for flexibility and predictable outputs. Vector indices are always sequential, thus you can be confident of which element is being processed, especially if it involves multiple inputs. The sequential nature of indices also mean that outputs are always in the order of the inputs, allowing you to safely join/bind them to existing data used as inputs. In addition, if you have data that contain extra information stored as attributes (discussed in next episode), you can safely re-assign them to the output for downstream use.</p>"},{"location":"4.iterations/#thy-can-grow-vectors-with-constraints","title":"Thy can grow vectors (with constraints)","text":"<p>As mentioned above, it is good practice to pre-assign vectors to store outputs of iterations (if desired). This ensures that R pre-allocates some amount of memory for the outputs. Whether or not it is a sufficient pre-allocation depends on the data structure of the outputs (see below). Even if the output vector is not strictly known (perhaps there are <code>if-else</code> statements that control which iteration is stored), an educated guess (with extra length for margin of error) is still better than nothing. Remember, <code>NA</code> can be filtered out later. Furthermore, a benefit of knowing (roughly) how many iterations required means that the index of the output can be used as the iterator.</p> <p>That being said, there may be situations where we truly do not know an appropriate length of an output vector. If that is the case, we should carefully construct loops with judicious use control flow elements like <code>if-else</code>, <code>while</code>, <code>next</code>, and <code>break</code>. Two reasonable scenarios where an empty vector can be grown (safely) is when you have control flow elements that (1) overwrite existing values (e.g., a counter or queue of sorts) or (2) control the size of the output vector. Growing vectors necessitates approaches that are position-agnostic. If you knew certain indices will produce certain outcomes, then you should've pre-assigned a vector of known length. </p> <p>If you think you will be processing a large number of iterations, need to store the results, and the output might exceed the size of available memory, consider coercing those into \"flat\" text formats (e.g., tab/comma separated values, strings, etc.) and writing it out to disk with a function that can open and append existing files (e.g., <code>readr::write_csv(x, append = TRUE)</code>). If you are going to do this, be mindful of the available disk space.</p>"},{"location":"4.iterations/#prefer-vectorised-functions","title":"Prefer vectorised functions","text":"<p>Functions that operate on the entirety of the vector input are called vectorised functions. These functions are optimised to run on entire data objects without explicitly iterating through the input element-by-element. This means that the processing is fast and efficient with minimal overheads. For example:</p> <p>code</p> <pre><code>num_1 &lt;- c(2, 4, 6, 8, 10)\nnum_2 &lt;- c(1, 3, 5, 7, 9)\n\n# Do this:\nsums_a &lt;- num_1 + num_2\n\n# Avoid this:\nsums_b &lt;- numeric(5)\nfor (i in 1:5) {\n  sums_b[i] &lt;- sum(num_1[i], num_2[i])\n}\n\n# The results are the same\nall(sums_a == sums_b)\n</code></pre> <p>Iterations in under the hood</p> <p>Technically, most vectorised functions are iterations. However, these are implemented in C. For many functions that come with base R, this C code is called immediately. This means the iterations are happening at the level closest to actual processing speeds. Explicit iterations like <code>for</code> loops in R require additional translation into machine code which is an overhead. While <code>map()</code> does implement iterations in C, there are some processes that happen at the R level before it reaches C-level code.</p>"},{"location":"4.iterations/#less-is-more","title":"Less is more","text":"<p>While iterations are helpful when you have a lot of data to go through, it can be computationally expensive. Try to reduce the work done within an iteration and use vectorised functions to achieve part of the task. Consider also if only part of data requires an iteration for analysis (i.e., pre-filtering the input data). If there are many steps that must be achieved using iteration, try to break those steps up into smaller iterations (i.e., modular code). When using iterations inside a function, it is even more important to exit iterations as quickly as possible so as to not stall if things go wrong (recall \"failing fast\").</p> <p>\"Less is more\" also applies to the results of iterations. If each iteration computes and produces voluminous intermediate outcomes (e.g., permutation, bootstrapping, imputation, etc.), consider if all those values are required for downstream analyses within the same session. It is understood that these intermediate values can be valuable for troubleshooting and understanding the underlying assumptions of the analysis. In that case, consider redirecting them into files.</p> <p>If you must iterate through a large amount of data, also consider using combining control flow elements like <code>if-else</code> with loop specific statements like <code>break</code> or <code>next</code> to make sure you are computing on elements and storing outputs that meet certain conditions.</p>"},{"location":"4.iterations/#capture-errors-return-results","title":"Capture errors, return results","text":"<p>There are situations where errors persist despite our best efforts to guard against them. Defensive programming applies to scripting as well! It can be a expensive to run large numbers of iterations only for it to be stopped and error out without saving other valid outputs. In that case, R has a helpful function: <code>try()</code>. We know our <code>calc_alpha_diversity()</code> will error out on non-numeric data, let's use that to demonstrate how <code>try()</code> can help save computation from other valid inputs:</p> <p>code</p> <pre><code>map(asv, \\(x) {\n  try({\n    calc_alpha_diversity(x, index = \"shannon\")\n  })\n})\n</code></pre>"},{"location":"4.iterations/#structural-and-analytical-considerations","title":"Structural and analytical considerations","text":"<p>How data is structured is often the most important consideration when deciding between using <code>for</code> loops and <code>map()</code>. If data and subsequent analyses are easily amenable to combinatorial methods like <code>combn()</code> or <code>expand.grid()</code>, we can make life easier by using the <code>map()</code> method. On the other hand, it is difficult (sometimes impossible) to coerce some data structures into \"flat\" tables without extensive modification (and code). For example, if the data is stored in a highly nested, complex, and uneven manner (e.g., XML, JSON, graph/tree structures), a <code>for</code> loop may be more appropriate, maintainable (AKA easy to debug), readable, and verbose. Some analyses are not amenable to \"flattening\", such as those that require updating objects in the global environment or is (conditionally) dependent on results from previous iterations. If your analyses necessitates non-trivial code for intensive computation (e.g., generates dependent intermediates, has complex control flow), a <code>for</code> loop is certainly more readable than an abstraction using <code>map()</code>. Remember, computation time is cheap compared to human time. Choose the best construct that will get the job done in reasonable time within hardware constraints.</p>"},{"location":"5.data_structures/","title":"Data structures","text":"<p>Learning objectives</p> <ul> <li>Understand data frames and tibbles are special lists</li> <li>Know when and why to use a matrix</li> <li>Understand attributes and how they affect data structure</li> <li>Understand recursion in functions</li> </ul> <p> <p><pre><code>graph BT\n  A[\"Vector\"]\n  B[\"Atomic (1-D)\"]\n  C[\"Matrix (2-D)\"]\n  D[\"Array (n-D)\"]\n  E[\"List\"]\n  F[\"Data frame\"]\n  G[\"Tibble\"]\n  B --- A\n  C --- B\n  D --- B\n  E --- A\n  F --- E\n  G --- E  </code></pre> </p> <p>All data objects are vectors. The difference between the colloquial vector (atomic vector) and list is the kinds of data they can contain: atomic vectors only store data of the same type in a single dimension (e.g., <code>c(1, 2, 3, 4, 5)</code>); lists are generic vectors that are able to store data of variable types and can also store other lists (i.e., recursion, see below). A further generalisation of the atomic vector are matrices (for 2-dimensional data) and arrays (for n-dimensions). While these types of data structures are seemingly restrictive (only one data mode allowed), they are how the vast majority of statistics and math is actually done!</p>"},{"location":"5.data_structures/#tabular-data","title":"Tabular data","text":"<p>In Introduction to R, we introduced the data frame as a form tabular data storage. Here, we will extend that knowledge by exploring some of the underlying principles of tabular data structures in R. Additionally, we will also learn how to use them effectively.</p>"},{"location":"5.data_structures/#data-frames-are-special-lists","title":"Data frames are special lists","text":"<p>Technically, data frames are special lists: they contain lists of variables of equal lengths (i.e., cases-by-variables). For practical purposes, think of each column as a vector, and that each vector has the same length that makes up the rows of the data frame. The following example exposes this construct:</p> <p>code</p> <pre><code>nr &lt;- 10\ndf_list &lt;- list(\n  \"A\" = sample(asv$ASVID, nr),\n  \"B\" = sample(asv[[sample(colnames(asv), 1)]], nr),\n  \"C\" = sample(c(TRUE, FALSE), nr, replace = TRUE)\n)\n\n# Print data.frame\nas.data.frame(df_list)\n\n# Check equivalence and structure\nis.list(as.data.frame(df_list)) == is.list(df_list)\n\nstr(as.data.frame(df_list))\nstr(df_list)\n</code></pre> <p><code>arguments imply differing number of rows</code></p> <p>If the number of elements of the vectors in the list is different, we will get the above error. This reinforces the special case of data frame as a list-based structure.</p> <p>Inspect source code of <code>data.frame</code></p> <p>If you look closely at the source code of the <code>data.frame</code> function, it exposes how the input is handled as a list and how it iteratively reconstructs the list into a tabular form, assigns column and row names, and performs checks on lengths.</p> <p>If we wanted to, we can also add row names for coercion:</p> <p>code</p> <pre><code>rn &lt;- sample(words, nr)\ndf_list &lt;- map(df_list, \\(x) {\n  names(x) &lt;- rn\n  x\n})\n\nstr(df_list)\nas.data.frame(df_list)\n</code></pre> <p>The list-based nature of the data frame means it is highly amenable to column-wise iteration via <code>map()</code> (or it's variants). However, due to the mixed data modes commonly found in data frames, the function being \"mapped\" will need to have some control flow statements to deal with them correctly.</p> <p>code</p> <pre><code># Shannon's index for each sample\nmap(asv, \\(x) {\n  if (is.numeric(x)) {\n    calc_alpha_diversity(x, index = \"shannon\")\n  }\n}) %&gt;%\n  unlist()\n</code></pre>"},{"location":"5.data_structures/#the-lazy-cousin-tibbles","title":"The lazy cousin: tibbles","text":"<p>Tibbles are a modern implementation of data frames developed and maintained by the folks behind tidyverse. We'll use an example to showcase how they are \"lazy\".</p> <p>code</p> <p>Create a data frame and a tibble.</p> <pre><code>df_A &lt;- data.frame(\n  a = 1:3,\n  b = I(matrix(rnorm(6), nrow = 3, ncol = 2)),\n  c = I(list(\n    matrix(rnorm(6), nrow = 3, ncol = 2),\n    matrix(rnorm(6), nrow = 3, ncol = 2),\n    as.data.frame(matrix(rnorm(6), nrow = 3, ncol = 2))\n    )\n  )\n)\n\ntbl_A &lt;- tibble(\n  a = 1:3,\n  `245` = 3:5,\n  `first letters` = letters[1:3],\n  b = matrix(rnorm(6), nrow = 3, ncol = 2),\n  c = list(\n    matrix(rnorm(6), nrow = 3, ncol = 2),\n    matrix(rnorm(6), nrow = 3, ncol = 2),\n    as.data.frame(matrix(rnorm(6), nrow = 3, ncol = 2))\n  )\n)\n</code></pre> Variables and their names are lazily evaluatedColumns are built sequentiallyEnhanced printingStrict subsetting <p>This means that variables are automatically interpreted \"as is\", meaning we can store non-atomic vectors within tibbles without extra effort (compare lines 3-9 to lines 14-19). Furthermore, column names with characters that are illegal in base R (i.e. with spaces) are returned as is. </p> <p>code</p> <pre><code>tbl_A$`245`\ntbl_A$`first letters`\n</code></pre> <p>Consider the following ways of obtaining the third column:</p> <p>code</p> <pre><code># Construct data frame\ndf_B &lt;- data.frame(\n  a = 1:5,\n  b = 5:9\n)\ndf_B$c &lt;- df_B$a * df_B$b\n\n# Construct tibble\ntbl_B &lt;- tibble(\n  a = 1:5,\n  b = 5:9,\n  c = a * b\n)\n</code></pre> <p>Both multiply columns <code>a</code> and <code>b</code> in the respective tables to obtain column <code>c</code>. However, notice how in <code>tibble()</code> this is done more naturally?</p> <p>code</p> <pre><code>print(asv)\nprint(as_tibble(asv))\nprint(as_tibble(asv), n = 15, width = 80)\n</code></pre> <p>Cleaner and informative output of a printed tibble compared to the data frame. Moreover, we can control how many lines (<code>n</code>) and number of characters (<code>width</code>) we want to see.</p> <p>When using single bracket notation to subset a tibble, the result is always a tibble. This is not the case with data frames. Compare the outputs below:</p> <p>code</p> <pre><code>df_A[, 1] %&gt;% str()\ntbl_A[, 1] %&gt;% str()\n</code></pre> <p>Row names are not a thing in tibbles!</p>"},{"location":"5.data_structures/#2-dimensional-superhero-matrices","title":"2-dimensional superhero: Matrices","text":"<p>The matrix is another class of tabular data object. They are the 2-dimensional generalisation of an atomic vector, which makes them simpler and stricter structures compared to data frames or tibbles. We'll explore some of the properties of matrices, and then discuss a key advantage they have over data frames.</p> Strict subsettingIterating on vector elements, not list of vectorsMemory usageMatrix operations <p>When using square bracket notation for subsetting matrices, the result is always an atomic vector if there is only one row OR column, or a matrix if more then 1 row AND column is desired.</p> <p>code</p> <pre><code>str(env_matrix[, 2])\nstr(env_matrix[2, ])\nstr(env_matrix[1:5, 2:4])\n</code></pre> <p>This property of strict subsetting and homogeneous data mode means that obtaining values from matrices is straight-forward and has predictable outputs. </p> <p>The atomic vector property of matrices means that iterations using <code>map()</code> is applied on each element. That is not the same as iterating on columns of a data frame. </p> <p>code</p> <pre><code>map(env, length)\nmap(env_matrix, length)\n</code></pre> <p>Furthermore, the code above nicely illustrates that the total length of the data object. Line 1 gives the length of each column in a data frame, which is equivalent to the length of each vector in a list. Line 2 considers the length of each element of an atomic vector, hence the output is 1 repeated in a list of length 231, the latter of which is the number of elements in the matrix.</p> <p>Use <code>apply()</code> to iterate correctly on a matrix</p> <p>If iterating along the margins (AKA rows or columns) of the matrix is required, use the base R function <code>apply()</code> and specify the margin desired.</p> <p>code</p> <pre><code># Iterating along rows\napply(env_matrix, MARGIN = 1, length)\n# Iterating along columns\napply(env_matrix, MARGIN = 2, length)\n</code></pre> <p>For storing and processing large amounts of homogeneous data, matrix should be the go to data storage class. Compared to data frames (and tibbles), matrices are more compact (occupy less RAM).</p> <p>code</p> <pre><code># Create data frame equivalent of asv_matrix\nasv_dataframe &lt;- asv[, -1]\nrownames(asv_dataframe) &lt;- asv$ASVID\n# Create tibble equivalent\nasv_tibble &lt;- as_tibble(asv_dataframe, rownames = NA)\n\nmap(\n  mget(ls(pattern = \"asv_.*), envir = .GlobalEnv),\n  object.size\n)        \n</code></pre> <p>The atomic vector property of matrices also lends itself to powerful matrix operations. These are mathematical operations that work on entire matrices (recall vectorised functions), and they are the basis of much of the underlying statistical processes in R (e.g., matrix multiplication <code>%*%</code>, transposition <code>t()</code>, eigenvector <code>eigen()</code>, etc.). For most statistical analyses, such as calculating correlations <code>cor()</code>, covariance <code>cov()</code>, distances <code>dist()</code>, principle component analysis <code>princomp()</code> etc., the first thing that those functions do is coerce the input data into a matrix. Beyond R, entire branches of STEM rely on matrix operations to obtain numerical solutions to complex, high-dimensional problems (e.g., quantum physics, multiple sequence alignments, the color filter applied to your Instagram posts).</p> <p>Due to the fundamental nature of matrix operations, they are implemented at the C/C++ level (available in the R source code or it's GitHub mirror). Higher level R functions (i.e., functions exposed to users) typically call those as soon as possible. This also means that users can exploit highly efficient and optimised algorithms packaged in Basic Linear Algebra Subprograms (BLAS) and Linear Algebra PACKage (LAPACK). Popular options include Intel's Math Kernel Library (optimised for Intel processors) and OpenBLAS. These software libraries provide substantial improvements in terms of computational efficiency, but are difficult to install for the average user (Thankfully, NeSI's R come pre-linked with BLAS and LAPACK libraries).</p>"},{"location":"5.data_structures/#attributes","title":"Attributes","text":"<p>Think of attributes as metadata to the object. Some attributes determine the structure of the data so that R can interpret them properly, other attributes are attached as metadata or for the purposes of provenance. Every time we perform some action on an object, the object's class attribute will be examined prior to evaluation as some operations are restricted to certain classes. Besides <code>class()</code>, other basic attributes include <code>names()</code>, dimensions <code>dim()</code>, and dimension names <code>dimnames()</code>. In the context of matrices, <code>dim()</code> and <code>dimnames()</code> (list of row and column names, in that specific order) are important attributes that differentiates it from a 1-dimensional atomic vector. </p> <p>To extract and modify attributes:</p> <p>code</p> <pre><code># Create example data\nenv_scaled &lt;- scale(env_matrix)\n\n# Inspect attributes\nattributes(env_scaled)\n\n# Extract attributes\nattributes(env_scaled)$`scaled:scale`\n</code></pre> <p>As observed, calling <code>attributes()</code> returns a list object of the available attributes from which we can inspect. As it is a list, we can modify and add attributes by assigning it like a list-element.</p> <p>code</p> <pre><code>attributes(env_scaled)$method &lt;- \"Z-score standardisation\"\n\n# Inspect attributes\nattributes(env_scaled)\n</code></pre>"},{"location":"5.data_structures/#recursive-objects-functions","title":"Recursive objects: Functions","text":"<p>In the Data Prelude lesson of Introduction to R, we were introduced to the list class of data storage and how to manipulate them. Lists are an example of a recursive data object. That means that they can store other objects of different modes and classes, thus allowing for complex and hierarchical structures through nesting (think Russian dolls). To check if something is recursive, we can use the predicate function <code>is.recursive()</code>.</p> <p>Something that may not be immediately intuitive is that functions are also recursive objects. In this context, recursion means something slightly different: a recursive function is a function that calls itself as part of it's source code/expression. For example, a factorial function applicable to integers:</p> \\[ n! = \\prod_{i=1}^{n} i \\] <p>code</p> <pre><code>calc_factorial &lt;- function(n) {\n  if (n &gt; 1) {\n    n * calc_factorial(n - 1)\n  } else {\n    1\n  }\n}\n\n# Test that it works\ncalc_factorial(4)\nprod(1, 2, 3, 4)\n</code></pre> <p>This function is the first time <code>calc_factorial()</code> is defined, but it calls itself in its expression! This is one of the examples of a recursive function. The recursive nature might be more easily understood as a diagram:</p> <pre><code>graph TD\n  F4(\"calc_factorial(4)\")\n  F3(\"calc_factorial(3)\")\n  F2(\"calc_factorial(2)\")\n  F1(\"calc_factorial(1)\")\n\n  F4 --&gt; 4\n  F4 --&gt; F3\n  F3 --&gt; 3\n  F3 --&gt; F2\n  F2 --&gt; 2\n  F2 --&gt; F1\n  F1 --&gt; 1</code></pre> <p>At the end of the diagram, we multiply all the tips (results) to get the solution.</p>"},{"location":"5.data_structures/#s3-and-s4-objects","title":"S3 and S4 objects","text":"<p>In R, objects obey different object-oriented programming systems (namely S3, S4, and R6) to define what the objects are. This primarily affects the object's class and how R interprets and processes the object. A large majority of the objects in R follow the S3 system.</p> <p>The major difference between the two is that S4 is formally defined and stricter than S3. S4 is generally preferred when building large packages or suites of interacting packages with complex methods so that outputs and methods are consistent and that all code contributors have the same \"vocabulary\" for handling objects. This is the reason that all packages in Bioconductor are written using the S4 system in mind.</p> <p>An additional item in S4 is slots, where slot values (think object property) are retrieved using package-specific accessor functions or the symbol <code>@</code>. The details that differentiates the systems are too detailed for the average R user and will not be covered here. For further reading, we refer you to the Object-Oriented Programming section of Advanced R</p>"},{"location":"Supplementary/s1.seq_prep/","title":"Sequence processing for Intermediate R workshop","text":"<p>Processing of 16S rRNA amplicon data from BioProject PRJNA608458 using QIIME2.</p>"},{"location":"Supplementary/s1.seq_prep/#1-sra-download-and-qiime-import","title":"1. SRA download and qiime import","text":"<p>Get SRA accessions from NCBI based on BioProject accession. Use SRAToolkit to download sequences in batch. Filter to only download field samples.</p> <p>code</p> <pre><code>mkdir -p seq_prep/1.fastq\ncd seq_prep\n\nmodule purge\nmodule load sratoolkit/3.0.2\n\ngrep -v \"[CNX]-\" SraRunInfo.csv \\\n  | cut -d ',' -f 1 \\\n  | grep \"SRR\" \\\n  | xargs -n 1 -P 8 fastq-dump --split-files --gzip --skip-technical --outdir 1.fastq/\n</code></pre> <p>Create Manifest file for QIIME import.</p> <p>code</p> <pre><code>grep -v \"[CNX]-\" SraRunInfo.csv \\\n  | cut -d ',' -f 1,30 \\\n  | sed '1d' \\\n  | awk 'BEGIN {FS=\",\"; OFS=\"\\t\"} {print $2,\"$PWD/1.fastq/\"$1\"_1.fastq.gz\",\"$PWD/1.fastq/\"$1\"_2.fastq.gz\"}' \\\n  | sed '1i sample-id\\tforward-absolute-filepath\\treverse-absolute-filepath' \\\n  &gt; sra_manifest.tsv\n</code></pre> <p>Import as QIIME archive.</p> <p>code</p> <pre><code>module purge\nmodule load QIIME2/2022.2\n\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path sra_manifest.tsv \\\n  --output-path 1.paired-end-demux.qza \\\n  --input-format PairedEndFastqManifestPhred33V2\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#2-join-paired-end-reads","title":"2. Join paired-end reads","text":"<p>code</p> <pre><code>qiime vsearch join-pairs \\\n  --i-demultiplexed-seqs 1.paired-end-demux.qza \\\n  --o-joined-sequences 2.paired-end-demux-joined.qza\n</code></pre> <p>Check quality.</p> <p>code</p> <pre><code>qiime demux summarize \\\n  --i-data 2.paired-end-demux-joined.qza \\\n  --o-visualization 2.paired-end-demux-joined.qzv\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#3-quality-trimming","title":"3. Quality trimming","text":"<p>code</p> <pre><code>qiime quality-filter q-score \\\n  --p-min-quality 30 \\\n  --i-demux 2.paired-end-demux-joined.qza \\\n  --o-filtered-sequences 3.paired-end-demux-joined-filtered.qza \\\n  --o-filter-stats 3.paired-end-demux-joined-filter-stats.qza\n</code></pre> <p>Check quality.</p> <p>code</p> <pre><code>qiime demux summarize \\\n  --i-data 3.paired-end-demux-joined-filtered.qza \\\n  --o-visualization 3.paired-end-demux-joined-filtered.qzv\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#4-denoise-with-deblur","title":"4. Denoise with Deblur","text":"<p>code</p> <pre><code>qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs 3.paired-end-demux-joined-filtered.qza \\\n  --p-trim-length 258 \\\n  --p-sample-stats \\\n  --o-representative-sequences 4.rep-seqs.qza \\\n  --o-table 4.table.qza \\\n  --o-stats 4.deblur-stats.qza \\\n  --verbose \\\n  --p-jobs-to-start 12 \\\n  --output-dir 4.deblur_outputs\n</code></pre> <p>Visually inspect statistics.</p> <p>code</p> <pre><code>qiime deblur visualize-stats \\\n  --i-deblur-stats 4.deblur-stats.qza \\\n  --o-visualization 4.deblur-stats.qzv\n</code></pre> <p>Summarize feature table stats.</p> <p>code</p> <pre><code>qiime feature-table summarize \\\n  --i-table 4.table.qza \\\n  --o-visualization 4.table.qzv\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#5-classify-taxonomy","title":"5. Classify taxonomy","text":"<p>Download SILVA SSU NR99 archive from QIIME's data resources.</p> <p>code</p> <pre><code>wget https://data.qiime2.org/2022.2/common/silva-138-99-seqs.qza\nwget https://data.qiime2.org/2022.2/common/silva-138-99-tax.qza\n</code></pre> <p>SLURM script for classifying taxonomy.</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=5.classify_taxonomy\n#SBATCH --account=nesi02659\n#SBATCH --time=8:00:00\n#SBATCH --mem=128GB\n#SBATCH --partition=milan\n#SBATCH --cpus-per-task=24\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz\n\n# Module\nmodule purge\nmodule load QIIME2/2022.2\n\n# In-silico PCR\nqiime feature-classifier extract-reads \\\n  --i-sequences silva-138-99-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGGGGTAA \\\n  --p-r-primer GGACTACNVGGGTWTCTAAT \\\n  --p-n-jobs $SLURM_CPUS_PER_TASK \\\n  --o-reads 5.ref-seqs.qza\n\n# Train classifier\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads 5.ref-seqs.qza \\\n  --i-reference-taxonomy silva-138-99-tax.qza \\\n  --o-classifier 5.classifier.qza\n\n# Annotate representative sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier 5.classifier.qza \\\n  --i-reads 4.rep-seqs.qza \\\n  --o-classification 5.taxonomy.qza \\\n  --p-n-jobs $SLURM_CPUS_PER_TASK\n\n# Tabulate taxonomy\nqiime metadata tabulate \\\n  --m-input-file 5.taxonomy.qza \\\n  --o-visualization 5.taxonomy.qzv\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#6-clean-up-tables","title":"6. Clean up tables","text":"<p>Remove taxon with Mitochondria or Chloroplast.</p> <p>code</p> <pre><code>qiime taxa filter-table \\\n  --i-table 4.table.qza \\\n  --i-taxonomy 5.taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table 6.filtered-table.qza\n</code></pre> <p>Visualize.</p> <p>code</p> <pre><code>qiime feature-table summarize \\\n  --i-table 6.filtered-table.qza \\\n  --o-visualization 6.filtered-table.qzv\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#6-create-phylogeny","title":"6. Create phylogeny","text":"<p>Create alignments and phylogenetic tree.</p> <p>code</p> <pre><code>qiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences 4.rep-seqs.qza \\\n  --output-dir 7.phylogeny \\\n  --p-n-threads 12\n</code></pre> <p>Generate phylogeny-informed distance matrices.</p> <p>code</p> <pre><code>qiime diversity-lib unweighted-unifrac \\\n  --i-phylogeny 7.phylogeny/rooted_tree.qza \\\n  --i-table 6.filtered-table.qza \\\n  --p-threads 8 \\\n  --o-distance-matrix 7.unweighted-unifrac.qza\n</code></pre> <p>code</p> <pre><code>qiime diversity-lib weighted-unifrac \\\n  --i-phylogeny 7.phylogeny/rooted_tree.qza \\\n  --i-table 6.filtered-table.qza \\\n  --p-threads 8 \\\n  --o-distance-matrix 7.weighted-unifrac.qza\n</code></pre>"},{"location":"Supplementary/s1.seq_prep/#7-export-tables","title":"7. Export tables","text":"<p>QIIME artifacts are Zip archives. Simply unzip.</p> <p>code</p> <pre><code># Everything was previously done in seq_prep/\ncd ../\n\nmkdir -p data/\n\nfor i in seq_prep/{5.taxonomy,6.filtered-table,7.unweighted-unifrac,7.weighted-unifrac}.qza; do\n  name=$(basename $i .qza)\n  qiime tools export \\\n    --input-path $i \\\n    --output-path data/$name\ndone\n</code></pre> <p>Convert BIOM feature table into TSV.</p> <p>code</p> <pre><code>biom convert \\\n  -i data/6.filtered-table/feature-table.biom \\\n  -o data/6.filtered-table/feature-table.tsv \\\n  --to-tsv\n</code></pre> <p>Clean up headers.</p> <p>code</p> <pre><code># Feature table\nsed '1d' data/6.filtered-table/feature-table.tsv \\\n  | sed 's/#OTU ID/ASVID/' \\\n  &gt; filtered_feature_table.tsv\n\n# Taxonomy table\nsed 's/ /_/' data/5.taxonomy/taxonomy.tsv &gt; taxonomy.tsv\n\n# Distance matrices\nfor i in data/7.*; do\n  name=$(basename $i .tsv | sed 's/-/_/g' | sed 's/7.//g')\n  echo $name\n  sed 's/^\\t/sample\\t/' $i/distance-matrix.tsv &gt; $name.tsv\ndone\n</code></pre>"},{"location":"Supplementary/s2.dat_prep/","title":"Data preparation","text":""},{"location":"Supplementary/s2.dat_prep/#1-prepare-working-environment","title":"1. Prepare working environment","text":"<p>Load required libraries and data.</p> <p>code</p> <pre><code># Load libraries\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(tidyr)\n\n# Import data\nasv &lt;- read_tsv(\"filtered_feature_table.tsv\")\nenv_data &lt;- read_tsv(\"sample_metadata.tsv\", name_repair = \"universal\")\nduu &lt;- read_tsv(\"unweighted_unifrac.tsv\")\ndwu &lt;- read_tsv(\"weighted_unifrac.tsv\")\n\n# Create output directory\ndir.create(\"./tables/\")\n</code></pre>"},{"location":"Supplementary/s2.dat_prep/#2-clean-environmental-metadata","title":"2. Clean environmental metadata","text":"<p>There is a block of samples that were originally part of a pilot project. The amount of sample matter collected were insufficient for subsequent chemical analyses. It was retained in the original study as a reference point to determine if there were random effects that would affect the data. For the purposes of a workshop, these samples without chemical data should be removed to streamline the curriculum.</p> <p>Cleaning tasks:</p> <ul> <li>Remove samples with NA in chemical data</li> <li>Arrange rows based on sample ID</li> <li>Convert column names to small letters</li> </ul> <p>code</p> <pre><code>env_clean &lt;- env_data %&gt;%\n  drop_na() %&gt;%\n  arrange(Sample) %&gt;%\n  rename_with(str_to_lower)\n</code></pre>"},{"location":"Supplementary/s2.dat_prep/#3-clean-asv-table","title":"3. Clean ASV table","text":"<p>Cleaning tasks:</p> <ul> <li>Retain samples based on <code>env_clean$sample</code></li> <li>Remove ASVs with row sums of zeroes</li> </ul> <p>code</p> <pre><code>asv_clean &lt;- asv %&gt;%\n  select(ASVID, all_of(env_clean$sample)) %&gt;%\n  filter(\n    rowSums(\n        select(., where(is.numeric))\n    ) &gt; 0\n  )\n</code></pre>"},{"location":"Supplementary/s2.dat_prep/#4-clean-distance-matrices","title":"4. Clean distance matrices","text":"<p>Cleaning tasks:</p> <ul> <li>Retain rows and columns based on <code>env_clean$sample</code></li> <li>Rearrange rows and columns based on <code>env_clean$sample</code></li> </ul> <p>code</p> <pre><code>duu_clean &lt;- duu %&gt;%\n  filter(sample %in% env_clean$sample) %&gt;%\n  select(sample, all_of(env_clean$sample)) %&gt;%\n  arrange(sample)\n\nall(diag(as.matrix(duu_clean[, -1])) == 0)\n\ndwu_clean &lt;- dwu %&gt;%\n  filter(sample %in% env_clean$sample) %&gt;%\n  select(sample, all_of(env_clean$sample)) %&gt;%\n  arrange(sample)\n\nall(diag(as.matrix(dwu_clean[, -1])) == 0)\n</code></pre>"},{"location":"Supplementary/s2.dat_prep/#write-out-cleaned-data","title":"Write out cleaned data","text":"<p>code</p> <pre><code>write_tsv(asv_clean, \"tables/asv_table.tsv\")\nwrite_tsv(env_clean, \"tables/env_table.tsv\")\nwrite_tsv(duu_clean, \"tables/unweighted_unifrac.dist\")\nwrite_tsv(dwu_clean, \"tables/weighted_unifrac.dist\")\n</code></pre> <p>Also, manually copy <code>taxonomy.tsv</code> into <code>tables/</code>. </p> <p>From now on, <code>tables/</code> is the source of data for lessons.</p>"}]}