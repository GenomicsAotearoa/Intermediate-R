{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"1.lesson_1/","text":"Set up \u00b6 Load libraries and import data. code library ( readr ) library ( dplyr ) library ( stringr ) asv <- read_tsv ( \"tables/asv_table.tsv\" ) taxa <- read_tsv ( \"tables/taxonomy.tsv\" ) env_data <- read_tsv ( \"tables/env_table.tsv\" ) Activity aims \u00b6 Create a taxonomy bar plot of relative abundances at Proteobacterial class level and phylum level for other phyla Conditional statements \u00b6 Learning objectives: Understand the if...else construct Understand that conditions can be chained sequentially The anatomy of the if..else : if (expression) { do_something() } else { do_something_else() } The if...else construct is a way of controlling output depending on a condition as evaluated using the expression inside the (...) . In other words, if the expression returns a TRUE , it will run the code within the first {...} , otherwise, it will run code in the second chunk {...} . code # Obtain total number of reads for a sample num_reads <- sum ( asv $ AS1A1 ) # Print output if the number of reads exceed 1000 if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } else { print ( \"Below 1000 reads\" ) } Output [1] \"Exceeds 1000 reads\" You can also omit the else part if you only want to do something if the condition is TRUE . The following statement will provide an output. code if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } Output [1] \"Exceeds 1000 reads\" However, this one will not. code if ( num_reads < 1000 ) { print ( \"Below 1000 reads\" ) } You can also chain multiple conditions together to generate different outputs depending on the results of each evaluated condition. code num_reads <- sum ( asv $ AS1A2 ) if ( num_reads > 5000 ) { print ( \"High biomass\" ) } else if ( num_reads >= 1000 & num_reads < 5000 ) { print ( \"Adequate biomass\" ) } else if ( num_reads < 1000 ) { print ( \"Inadequate biomass\" ) } else { print ( \"Unknown\" ) } Output [1] \"Adequate biomass\" When chaining multiple conditions, be aware of the order of the conditions. This is because conditions are evaluated sequentially. For example: code if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } else if ( num_reads >= 1000 & num_reads < 5000 ) { print ( \"Adequate biomass\" ) } else { print ( \"Unknown\" ) } Output [1] \"Exceeds 1000 reads\" Note that both the first and second expressions are TRUE , but only the first code chunk ran. This can become important if you need to run different code depending on value ranges. Here, the priority of conditions is paramount to how you need to construct your if...else statements. String manipulation \u00b6 Learning objectives: Detect patterns in strings Subset strings based on pattern Extract substrings based on pattern Replace substrings based on pattern Be aware of convenience functions Knowing how to detect, subset, and replace strings is helpful for quick checks and at later stages when you need to plot your data. The Taxon column in the taxa data frame is a helpful example for learning how to manipulate strings. This column consists of taxonomy assigned to each ASV (or feature). Glance at it using RStudio's viewer, you may notice that ASVs can be assigned taxonomy at any level (i.e., some ASVs are identified up to species level, some are identified only up to domain level). Let's start with detecting strings. code # Count number ASVs assigned as Archaea sum ( str_detect ( string = taxa $ Taxon , pattern = \"Archaea\" )) Output [1] 33 In the code above, str_detect() returns a logical vector, and sum() counts the number of TRUE elements. Now that we know we have 33 Archaeal ASVs, we might be interested in their lineage. code archaea_lineage <- str_subset ( string = taxa $ Taxon , pattern = \"Archaea\" ) # Get de-duplicated lineages unique ( archaea_lineage ) Output [1] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopumilus\" [2] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopelagicus; s__uncultured_archaeon\" [3] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Nitrosopumilaceae\" [4] \"d__Archaea;_p__Crenarchaeota; c__Bathyarchaeia; o__Bathyarchaeia; f__Bathyarchaeia; g__Bathyarchaeia; s__uncultured_miscellaneous\" [5] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopumilus; s__Candidatus_Nitrosopumilus\" [6] \"d__Archaea;_p__Crenarchaeota; c__Bathyarchaeia; o__Bathyarchaeia; f__Bathyarchaeia; g__Bathyarchaeia\" Using unique() , we see that we have Archaeal representatives that are all part of the phylum Crenarchaeota and class Nitrososphaeria and Bathyarchaeia. We can also extract part of a string. Here is an example where we extract \"Nitrososphaeria\" from the archaea_lineage . Joins \u00b6 Learning objectives: Understand the use of different join_*() Use join to combine tables based on different columns Loops for iteration \u00b6 Learning objectives: Understand the for construct Understand the difference between iterating vectors directly or their indices","title":"Set up"},{"location":"1.lesson_1/#set-up","text":"Load libraries and import data. code library ( readr ) library ( dplyr ) library ( stringr ) asv <- read_tsv ( \"tables/asv_table.tsv\" ) taxa <- read_tsv ( \"tables/taxonomy.tsv\" ) env_data <- read_tsv ( \"tables/env_table.tsv\" )","title":"Set up"},{"location":"1.lesson_1/#activity-aims","text":"Create a taxonomy bar plot of relative abundances at Proteobacterial class level and phylum level for other phyla","title":"Activity aims"},{"location":"1.lesson_1/#conditional-statements","text":"Learning objectives: Understand the if...else construct Understand that conditions can be chained sequentially The anatomy of the if..else : if (expression) { do_something() } else { do_something_else() } The if...else construct is a way of controlling output depending on a condition as evaluated using the expression inside the (...) . In other words, if the expression returns a TRUE , it will run the code within the first {...} , otherwise, it will run code in the second chunk {...} . code # Obtain total number of reads for a sample num_reads <- sum ( asv $ AS1A1 ) # Print output if the number of reads exceed 1000 if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } else { print ( \"Below 1000 reads\" ) } Output [1] \"Exceeds 1000 reads\" You can also omit the else part if you only want to do something if the condition is TRUE . The following statement will provide an output. code if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } Output [1] \"Exceeds 1000 reads\" However, this one will not. code if ( num_reads < 1000 ) { print ( \"Below 1000 reads\" ) } You can also chain multiple conditions together to generate different outputs depending on the results of each evaluated condition. code num_reads <- sum ( asv $ AS1A2 ) if ( num_reads > 5000 ) { print ( \"High biomass\" ) } else if ( num_reads >= 1000 & num_reads < 5000 ) { print ( \"Adequate biomass\" ) } else if ( num_reads < 1000 ) { print ( \"Inadequate biomass\" ) } else { print ( \"Unknown\" ) } Output [1] \"Adequate biomass\" When chaining multiple conditions, be aware of the order of the conditions. This is because conditions are evaluated sequentially. For example: code if ( num_reads > 1000 ) { print ( \"Exceeds 1000 reads\" ) } else if ( num_reads >= 1000 & num_reads < 5000 ) { print ( \"Adequate biomass\" ) } else { print ( \"Unknown\" ) } Output [1] \"Exceeds 1000 reads\" Note that both the first and second expressions are TRUE , but only the first code chunk ran. This can become important if you need to run different code depending on value ranges. Here, the priority of conditions is paramount to how you need to construct your if...else statements.","title":"Conditional statements"},{"location":"1.lesson_1/#string-manipulation","text":"Learning objectives: Detect patterns in strings Subset strings based on pattern Extract substrings based on pattern Replace substrings based on pattern Be aware of convenience functions Knowing how to detect, subset, and replace strings is helpful for quick checks and at later stages when you need to plot your data. The Taxon column in the taxa data frame is a helpful example for learning how to manipulate strings. This column consists of taxonomy assigned to each ASV (or feature). Glance at it using RStudio's viewer, you may notice that ASVs can be assigned taxonomy at any level (i.e., some ASVs are identified up to species level, some are identified only up to domain level). Let's start with detecting strings. code # Count number ASVs assigned as Archaea sum ( str_detect ( string = taxa $ Taxon , pattern = \"Archaea\" )) Output [1] 33 In the code above, str_detect() returns a logical vector, and sum() counts the number of TRUE elements. Now that we know we have 33 Archaeal ASVs, we might be interested in their lineage. code archaea_lineage <- str_subset ( string = taxa $ Taxon , pattern = \"Archaea\" ) # Get de-duplicated lineages unique ( archaea_lineage ) Output [1] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopumilus\" [2] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopelagicus; s__uncultured_archaeon\" [3] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Nitrosopumilaceae\" [4] \"d__Archaea;_p__Crenarchaeota; c__Bathyarchaeia; o__Bathyarchaeia; f__Bathyarchaeia; g__Bathyarchaeia; s__uncultured_miscellaneous\" [5] \"d__Archaea;_p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopumilus; s__Candidatus_Nitrosopumilus\" [6] \"d__Archaea;_p__Crenarchaeota; c__Bathyarchaeia; o__Bathyarchaeia; f__Bathyarchaeia; g__Bathyarchaeia\" Using unique() , we see that we have Archaeal representatives that are all part of the phylum Crenarchaeota and class Nitrososphaeria and Bathyarchaeia. We can also extract part of a string. Here is an example where we extract \"Nitrososphaeria\" from the archaea_lineage .","title":"String manipulation"},{"location":"1.lesson_1/#joins","text":"Learning objectives: Understand the use of different join_*() Use join to combine tables based on different columns","title":"Joins"},{"location":"1.lesson_1/#loops-for-iteration","text":"Learning objectives: Understand the for construct Understand the difference between iterating vectors directly or their indices","title":"Loops for iteration"},{"location":"Supplementary/s1.seq_prep/","text":"Sequence processing for Intermediate R workshop \u00b6 Processing of 16S rRNA amplicon data from BioProject PRJNA608458 using QIIME2. 1. SRA download and qiime import \u00b6 Get SRA accessions from NCBI based on BioProject accession. Use SRAToolkit to download sequences in batch. Filter to only download field samples. code mkdir -p seq_prep/1.fastq cd seq_prep module purge module load sratoolkit/3.0.2 grep -v \"[CNX]-\" SraRunInfo.csv \\ | cut -d ',' -f 1 \\ | grep \"SRR\" \\ | xargs -n 1 -P 8 fastq-dump --split-files --gzip --skip-technical --outdir 1 .fastq/ Create Manifest file for QIIME import. code grep -v \"[CNX]-\" SraRunInfo.csv \\ | cut -d ',' -f 1 ,30 \\ | sed '1d' \\ | awk 'BEGIN {FS=\",\"; OFS=\"\\t\"} {print $2,\"$PWD/1.fastq/\"$1\"_1.fastq.gz\",\"$PWD/1.fastq/\"$1\"_2.fastq.gz\"}' \\ | sed '1i sample-id\\tforward-absolute-filepath\\treverse-absolute-filepath' \\ > sra_manifest.tsv Import as QIIME archive. code module purge module load QIIME2/2022.2 qiime tools import \\ --type 'SampleData[PairedEndSequencesWithQuality]' \\ --input-path sra_manifest.tsv \\ --output-path 1 .paired-end-demux.qza \\ --input-format PairedEndFastqManifestPhred33V2 2. Join paired-end reads \u00b6 code qiime vsearch join-pairs \\ --i-demultiplexed-seqs 1 .paired-end-demux.qza \\ --o-joined-sequences 2 .paired-end-demux-joined.qza Check quality. code qiime demux summarize \\ --i-data 2 .paired-end-demux-joined.qza \\ --o-visualization 2 .paired-end-demux-joined.qzv 3. Quality trimming \u00b6 code qiime quality-filter q-score \\ --p-min-quality 30 \\ --i-demux 2 .paired-end-demux-joined.qza \\ --o-filtered-sequences 3 .paired-end-demux-joined-filtered.qza \\ --o-filter-stats 3 .paired-end-demux-joined-filter-stats.qza Check quality. code qiime demux summarize \\ --i-data 3 .paired-end-demux-joined-filtered.qza \\ --o-visualization 3 .paired-end-demux-joined-filtered.qzv 4. Denoise with Deblur \u00b6 code qiime deblur denoise-16S \\ --i-demultiplexed-seqs 3 .paired-end-demux-joined-filtered.qza \\ --p-trim-length 258 \\ --p-sample-stats \\ --o-representative-sequences 4 .rep-seqs.qza \\ --o-table 4 .table.qza \\ --o-stats 4 .deblur-stats.qza \\ --verbose \\ --p-jobs-to-start 12 \\ --output-dir 4 .deblur_outputs Visually inspect statistics. code qiime deblur visualize-stats \\ --i-deblur-stats 4 .deblur-stats.qza \\ --o-visualization 4 .deblur-stats.qzv Summarize feature table stats. code qiime feature-table summarize \\ --i-table 4 .table.qza \\ --o-visualization 4 .table.qzv 5. Classify taxonomy \u00b6 Download SILVA SSU NR99 archive from QIIME's data resources. code wget https://data.qiime2.org/2022.2/common/silva-138-99-seqs.qza wget https://data.qiime2.org/2022.2/common/silva-138-99-tax.qza SLURM script for classifying taxonomy. code #!/bin/bash -e #SBATCH --job-name=5.classify_taxonomy #SBATCH --account=nesi02659 #SBATCH --time=8:00:00 #SBATCH --mem=128GB #SBATCH --partition=milan #SBATCH --cpus-per-task=24 #SBATCH --output=%x.%j.out #SBATCH --error=%x.%j.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz # Module module purge module load QIIME2/2022.2 # In-silico PCR qiime feature-classifier extract-reads \\ --i-sequences silva-138-99-seqs.qza \\ --p-f-primer GTGYCAGCMGCCGGGGTAA \\ --p-r-primer GGACTACNVGGGTWTCTAAT \\ --p-n-jobs $SLURM_CPUS_PER_TASK \\ --o-reads 5 .ref-seqs.qza # Train classifier qiime feature-classifier fit-classifier-naive-bayes \\ --i-reference-reads 5 .ref-seqs.qza \\ --i-reference-taxonomy silva-138-99-tax.qza \\ --o-classifier 5 .classifier.qza # Annotate representative sequences qiime feature-classifier classify-sklearn \\ --i-classifier 5 .classifier.qza \\ --i-reads 4 .rep-seqs.qza \\ --o-classification 5 .taxonomy.qza \\ --p-n-jobs $SLURM_CPUS_PER_TASK # Tabulate taxonomy qiime metadata tabulate \\ --m-input-file 5 .taxonomy.qza \\ --o-visualization 5 .taxonomy.qzv 6. Clean up tables \u00b6 Remove taxon with Mitochondria or Chloroplast. code qiime taxa filter-table \\ --i-table 4 .table.qza \\ --i-taxonomy 5 .taxonomy.qza \\ --p-exclude mitochondria,chloroplast \\ --o-filtered-table 6 .filtered-table.qza Visualize. code qiime feature-table summarize \\ --i-table 6 .filtered-table.qza \\ --o-visualization 6 .filtered-table.qzv 6. Create phylogeny \u00b6 Create alignments and phylogenetic tree. code qiime phylogeny align-to-tree-mafft-fasttree \\ --i-sequences 4 .rep-seqs.qza \\ --output-dir 7 .phylogeny \\ --p-n-threads 12 Generate phylogeny-informed distance matrices. code qiime diversity-lib unweighted-unifrac \\ --i-phylogeny 7 .phylogeny/rooted_tree.qza \\ --i-table 6 .filtered-table.qza \\ --p-threads 8 \\ --o-distance-matrix 7 .unweighted-unifrac.qza code qiime diversity-lib weighted-unifrac \\ --i-phylogeny 7 .phylogeny/rooted_tree.qza \\ --i-table 6 .filtered-table.qza \\ --p-threads 8 \\ --o-distance-matrix 7 .weighted-unifrac.qza 7. Export tables \u00b6 QIIME artifacts are Zip archives. Simply unzip. code # Everything was previously done in seq_prep/ cd ../ mkdir -p data/ for i in seq_prep/ { 5 .taxonomy,6.filtered-table,7.unweighted-unifrac,7.weighted-unifrac } .qza ; do name = $( basename $i .qza ) qiime tools export \\ --input-path $i \\ --output-path data/ $name done Convert BIOM feature table into TSV. code biom convert \\ -i data/6.filtered-table/feature-table.biom \\ -o data/6.filtered-table/feature-table.tsv \\ --to-tsv Clean up headers. code # Feature table sed '1d' data/6.filtered-table/feature-table.tsv \\ | sed 's/#OTU ID/ASVID/' \\ > filtered_feature_table.tsv # Taxonomy table sed 's/ /_/' data/5.taxonomy/taxonomy.tsv > taxonomy.tsv # Distance matrices for i in data/7.* ; do name = $( basename $i .tsv | sed 's/-/_/g' | sed 's/7.//g' ) echo $name sed 's/^\\t/sample\\t/' $i /distance-matrix.tsv > $name .tsv done","title":"Sequence processing for Intermediate R workshop"},{"location":"Supplementary/s1.seq_prep/#sequence-processing-for-intermediate-r-workshop","text":"Processing of 16S rRNA amplicon data from BioProject PRJNA608458 using QIIME2.","title":"Sequence processing for Intermediate R workshop"},{"location":"Supplementary/s1.seq_prep/#1-sra-download-and-qiime-import","text":"Get SRA accessions from NCBI based on BioProject accession. Use SRAToolkit to download sequences in batch. Filter to only download field samples. code mkdir -p seq_prep/1.fastq cd seq_prep module purge module load sratoolkit/3.0.2 grep -v \"[CNX]-\" SraRunInfo.csv \\ | cut -d ',' -f 1 \\ | grep \"SRR\" \\ | xargs -n 1 -P 8 fastq-dump --split-files --gzip --skip-technical --outdir 1 .fastq/ Create Manifest file for QIIME import. code grep -v \"[CNX]-\" SraRunInfo.csv \\ | cut -d ',' -f 1 ,30 \\ | sed '1d' \\ | awk 'BEGIN {FS=\",\"; OFS=\"\\t\"} {print $2,\"$PWD/1.fastq/\"$1\"_1.fastq.gz\",\"$PWD/1.fastq/\"$1\"_2.fastq.gz\"}' \\ | sed '1i sample-id\\tforward-absolute-filepath\\treverse-absolute-filepath' \\ > sra_manifest.tsv Import as QIIME archive. code module purge module load QIIME2/2022.2 qiime tools import \\ --type 'SampleData[PairedEndSequencesWithQuality]' \\ --input-path sra_manifest.tsv \\ --output-path 1 .paired-end-demux.qza \\ --input-format PairedEndFastqManifestPhred33V2","title":"1. SRA download and qiime import"},{"location":"Supplementary/s1.seq_prep/#2-join-paired-end-reads","text":"code qiime vsearch join-pairs \\ --i-demultiplexed-seqs 1 .paired-end-demux.qza \\ --o-joined-sequences 2 .paired-end-demux-joined.qza Check quality. code qiime demux summarize \\ --i-data 2 .paired-end-demux-joined.qza \\ --o-visualization 2 .paired-end-demux-joined.qzv","title":"2. Join paired-end reads"},{"location":"Supplementary/s1.seq_prep/#3-quality-trimming","text":"code qiime quality-filter q-score \\ --p-min-quality 30 \\ --i-demux 2 .paired-end-demux-joined.qza \\ --o-filtered-sequences 3 .paired-end-demux-joined-filtered.qza \\ --o-filter-stats 3 .paired-end-demux-joined-filter-stats.qza Check quality. code qiime demux summarize \\ --i-data 3 .paired-end-demux-joined-filtered.qza \\ --o-visualization 3 .paired-end-demux-joined-filtered.qzv","title":"3. Quality trimming"},{"location":"Supplementary/s1.seq_prep/#4-denoise-with-deblur","text":"code qiime deblur denoise-16S \\ --i-demultiplexed-seqs 3 .paired-end-demux-joined-filtered.qza \\ --p-trim-length 258 \\ --p-sample-stats \\ --o-representative-sequences 4 .rep-seqs.qza \\ --o-table 4 .table.qza \\ --o-stats 4 .deblur-stats.qza \\ --verbose \\ --p-jobs-to-start 12 \\ --output-dir 4 .deblur_outputs Visually inspect statistics. code qiime deblur visualize-stats \\ --i-deblur-stats 4 .deblur-stats.qza \\ --o-visualization 4 .deblur-stats.qzv Summarize feature table stats. code qiime feature-table summarize \\ --i-table 4 .table.qza \\ --o-visualization 4 .table.qzv","title":"4. Denoise with Deblur"},{"location":"Supplementary/s1.seq_prep/#5-classify-taxonomy","text":"Download SILVA SSU NR99 archive from QIIME's data resources. code wget https://data.qiime2.org/2022.2/common/silva-138-99-seqs.qza wget https://data.qiime2.org/2022.2/common/silva-138-99-tax.qza SLURM script for classifying taxonomy. code #!/bin/bash -e #SBATCH --job-name=5.classify_taxonomy #SBATCH --account=nesi02659 #SBATCH --time=8:00:00 #SBATCH --mem=128GB #SBATCH --partition=milan #SBATCH --cpus-per-task=24 #SBATCH --output=%x.%j.out #SBATCH --error=%x.%j.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz # Module module purge module load QIIME2/2022.2 # In-silico PCR qiime feature-classifier extract-reads \\ --i-sequences silva-138-99-seqs.qza \\ --p-f-primer GTGYCAGCMGCCGGGGTAA \\ --p-r-primer GGACTACNVGGGTWTCTAAT \\ --p-n-jobs $SLURM_CPUS_PER_TASK \\ --o-reads 5 .ref-seqs.qza # Train classifier qiime feature-classifier fit-classifier-naive-bayes \\ --i-reference-reads 5 .ref-seqs.qza \\ --i-reference-taxonomy silva-138-99-tax.qza \\ --o-classifier 5 .classifier.qza # Annotate representative sequences qiime feature-classifier classify-sklearn \\ --i-classifier 5 .classifier.qza \\ --i-reads 4 .rep-seqs.qza \\ --o-classification 5 .taxonomy.qza \\ --p-n-jobs $SLURM_CPUS_PER_TASK # Tabulate taxonomy qiime metadata tabulate \\ --m-input-file 5 .taxonomy.qza \\ --o-visualization 5 .taxonomy.qzv","title":"5. Classify taxonomy"},{"location":"Supplementary/s1.seq_prep/#6-clean-up-tables","text":"Remove taxon with Mitochondria or Chloroplast. code qiime taxa filter-table \\ --i-table 4 .table.qza \\ --i-taxonomy 5 .taxonomy.qza \\ --p-exclude mitochondria,chloroplast \\ --o-filtered-table 6 .filtered-table.qza Visualize. code qiime feature-table summarize \\ --i-table 6 .filtered-table.qza \\ --o-visualization 6 .filtered-table.qzv","title":"6. Clean up tables"},{"location":"Supplementary/s1.seq_prep/#6-create-phylogeny","text":"Create alignments and phylogenetic tree. code qiime phylogeny align-to-tree-mafft-fasttree \\ --i-sequences 4 .rep-seqs.qza \\ --output-dir 7 .phylogeny \\ --p-n-threads 12 Generate phylogeny-informed distance matrices. code qiime diversity-lib unweighted-unifrac \\ --i-phylogeny 7 .phylogeny/rooted_tree.qza \\ --i-table 6 .filtered-table.qza \\ --p-threads 8 \\ --o-distance-matrix 7 .unweighted-unifrac.qza code qiime diversity-lib weighted-unifrac \\ --i-phylogeny 7 .phylogeny/rooted_tree.qza \\ --i-table 6 .filtered-table.qza \\ --p-threads 8 \\ --o-distance-matrix 7 .weighted-unifrac.qza","title":"6. Create phylogeny"},{"location":"Supplementary/s1.seq_prep/#7-export-tables","text":"QIIME artifacts are Zip archives. Simply unzip. code # Everything was previously done in seq_prep/ cd ../ mkdir -p data/ for i in seq_prep/ { 5 .taxonomy,6.filtered-table,7.unweighted-unifrac,7.weighted-unifrac } .qza ; do name = $( basename $i .qza ) qiime tools export \\ --input-path $i \\ --output-path data/ $name done Convert BIOM feature table into TSV. code biom convert \\ -i data/6.filtered-table/feature-table.biom \\ -o data/6.filtered-table/feature-table.tsv \\ --to-tsv Clean up headers. code # Feature table sed '1d' data/6.filtered-table/feature-table.tsv \\ | sed 's/#OTU ID/ASVID/' \\ > filtered_feature_table.tsv # Taxonomy table sed 's/ /_/' data/5.taxonomy/taxonomy.tsv > taxonomy.tsv # Distance matrices for i in data/7.* ; do name = $( basename $i .tsv | sed 's/-/_/g' | sed 's/7.//g' ) echo $name sed 's/^\\t/sample\\t/' $i /distance-matrix.tsv > $name .tsv done","title":"7. Export tables"},{"location":"Supplementary/s2.dat_prep/","text":"Data preparation \u00b6 1. Prepare working environment \u00b6 Load required libraries and data. code # Load libraries library ( dplyr ) library ( stringr ) library ( readr ) library ( tidyr ) # Import data asv <- read_tsv ( \"filtered_feature_table.tsv\" ) env_data <- read_tsv ( \"sample_metadata.tsv\" , name_repair = \"universal\" ) duu <- read_tsv ( \"unweighted_unifrac.tsv\" ) dwu <- read_tsv ( \"weighted_unifrac.tsv\" ) # Create output directory dir.create ( \"./tables/\" ) 2. Clean environmental metadata \u00b6 There is a block of samples that were originally part of a pilot project. The amount of sample matter collected were insufficient for subsequent chemical analyses. It was retained in the original study as a reference point to determine if there were random effects that would affect the data. For the purposes of a workshop, these samples without chemical data should be removed to streamline the curriculum. Cleaning tasks: Remove samples with NA in chemical data Arrange rows based on sample ID Convert column names to small letters code env_clean <- env_data %>% drop_na () %>% arrange ( Sample ) %>% rename_with ( str_to_lower ) 3. Clean ASV table \u00b6 Cleaning tasks: Retain samples based on env_clean$sample Remove ASVs with row sums of zeroes code asv_clean <- asv %>% select ( ASVID , all_of ( env_clean $ sample )) %>% filter ( rowSums ( select ( . , where ( is.numeric )) ) > 0 ) 4. Clean distance matrices \u00b6 Cleaning tasks: Retain rows and columns based on env_clean$sample Rearrange rows and columns based on env_clean$sample code duu_clean <- duu %>% filter ( sample %in% env_clean $ sample ) %>% select ( sample , all_of ( env_clean $ sample )) %>% arrange ( sample ) all ( diag ( as.matrix ( duu_clean [, -1 ])) == 0 ) dwu_clean <- dwu %>% filter ( sample %in% env_clean $ sample ) %>% select ( sample , all_of ( env_clean $ sample )) %>% arrange ( sample ) all ( diag ( as.matrix ( dwu_clean [, -1 ])) == 0 ) Write out cleaned data \u00b6 code write_tsv ( asv_clean , \"tables/asv_table.tsv\" ) write_tsv ( env_clean , \"tables/env_table.tsv\" ) write_tsv ( duu_clean , \"tables/unweighted_unifrac.dist\" ) write_tsv ( dwu_clean , \"tables/weighted_unifrac.dist\" ) Also, manually copy taxonomy.tsv into tables/ . From now on, tables/ is the source of data for lessons.","title":"Data preparation"},{"location":"Supplementary/s2.dat_prep/#data-preparation","text":"","title":"Data preparation"},{"location":"Supplementary/s2.dat_prep/#1-prepare-working-environment","text":"Load required libraries and data. code # Load libraries library ( dplyr ) library ( stringr ) library ( readr ) library ( tidyr ) # Import data asv <- read_tsv ( \"filtered_feature_table.tsv\" ) env_data <- read_tsv ( \"sample_metadata.tsv\" , name_repair = \"universal\" ) duu <- read_tsv ( \"unweighted_unifrac.tsv\" ) dwu <- read_tsv ( \"weighted_unifrac.tsv\" ) # Create output directory dir.create ( \"./tables/\" )","title":"1. Prepare working environment"},{"location":"Supplementary/s2.dat_prep/#2-clean-environmental-metadata","text":"There is a block of samples that were originally part of a pilot project. The amount of sample matter collected were insufficient for subsequent chemical analyses. It was retained in the original study as a reference point to determine if there were random effects that would affect the data. For the purposes of a workshop, these samples without chemical data should be removed to streamline the curriculum. Cleaning tasks: Remove samples with NA in chemical data Arrange rows based on sample ID Convert column names to small letters code env_clean <- env_data %>% drop_na () %>% arrange ( Sample ) %>% rename_with ( str_to_lower )","title":"2. Clean environmental metadata"},{"location":"Supplementary/s2.dat_prep/#3-clean-asv-table","text":"Cleaning tasks: Retain samples based on env_clean$sample Remove ASVs with row sums of zeroes code asv_clean <- asv %>% select ( ASVID , all_of ( env_clean $ sample )) %>% filter ( rowSums ( select ( . , where ( is.numeric )) ) > 0 )","title":"3. Clean ASV table"},{"location":"Supplementary/s2.dat_prep/#4-clean-distance-matrices","text":"Cleaning tasks: Retain rows and columns based on env_clean$sample Rearrange rows and columns based on env_clean$sample code duu_clean <- duu %>% filter ( sample %in% env_clean $ sample ) %>% select ( sample , all_of ( env_clean $ sample )) %>% arrange ( sample ) all ( diag ( as.matrix ( duu_clean [, -1 ])) == 0 ) dwu_clean <- dwu %>% filter ( sample %in% env_clean $ sample ) %>% select ( sample , all_of ( env_clean $ sample )) %>% arrange ( sample ) all ( diag ( as.matrix ( dwu_clean [, -1 ])) == 0 )","title":"4. Clean distance matrices"},{"location":"Supplementary/s2.dat_prep/#write-out-cleaned-data","text":"code write_tsv ( asv_clean , \"tables/asv_table.tsv\" ) write_tsv ( env_clean , \"tables/env_table.tsv\" ) write_tsv ( duu_clean , \"tables/unweighted_unifrac.dist\" ) write_tsv ( dwu_clean , \"tables/weighted_unifrac.dist\" ) Also, manually copy taxonomy.tsv into tables/ . From now on, tables/ is the source of data for lessons.","title":"Write out cleaned data"}]}